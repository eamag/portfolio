{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.optimize import minimize, fmin_slsqp\n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and sub format already merged with weather and holidays (plus the day before holiday)\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['Timestamp','ForecastId','Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = time_preprocess(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ForecastId</th>\n",
       "      <th>Value</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>91600</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>136500</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>335400</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>379000</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>344100</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  ForecastId   Value  year  month  day  week_day  hour  minute\n",
       "0 2015-01-01           0   91600  2015      1    1         3     0       0\n",
       "1 2015-01-02           0  136500  2015      1    2         4     0       0\n",
       "2 2015-01-03           0  335400  2015      1    3         5     0       0\n",
       "3 2015-01-04           0  379000  2015      1    4         6     0       0\n",
       "4 2015-01-05           0  344100  2015      1    5         0     0       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanEncodingTransforming(X, y, X_test, how_to_fill):\n",
    "    \n",
    "    # mean encoding for lgb\n",
    "    \n",
    "    X_train = pd.concat([X, y], axis=1)\n",
    "    mean_values = X_train.groupby(X_train.columns[0]).agg(how_to_fill).to_dict()['Value']\n",
    "    X_train = X_train.drop(y.columns[0], axis=1)\n",
    "    X_train = X_train.replace(mean_values)\n",
    "    X_test = X_test.replace(mean_values)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    return X_train, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_groupby(train, test, window, how):\n",
    "    \n",
    "    # simple groupby prediction \n",
    "    \n",
    "    # time_delta = list((test['Timestamp'].iloc[-1:]  - train['Timestamp'].iloc[1] ).dt.days)[0]\n",
    "\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean(train, window):\n",
    "    \n",
    "    # return mean value from train for the window \n",
    "    \n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "   \n",
    "    return mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lgb(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    d2 = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, valid_sets=d2, \n",
    "                    num_boost_round=50000, early_stopping_rounds=100)\n",
    "    \n",
    "    y_hat = gbm.predict(X_valid)\n",
    "    opt_boost_rounds = gbm.best_iteration\n",
    "    \n",
    "    return y_hat, opt_boost_rounds \n",
    "\n",
    "\n",
    "\n",
    "def train_lgb(X_train, y_train, X_test, opt_boost_rounds):\n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, num_boost_round=opt_boost_rounds)\n",
    "    \n",
    "    y_hat = gbm.predict(X_test)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=142, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_y_hats(y_hats):\n",
    "    X_stack = pd.DataFrame({})\n",
    "    for i in range(0, len(y_hats)):\n",
    "        X_stack['stack'+str(i)] = y_hats[i]\n",
    "    return X_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stack(X_stack, y, model):\n",
    "    model.fit(X_stack, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_harmonic_features(x, col, period=24):\n",
    "    x['sin_'+col] = np.sin(x[col] * 2 * np.pi / period)\n",
    "    x['cos_'+col] = np.cos(x[col] * 2 * np.pi / period)\n",
    "    x = x.drop(col, axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minutes_in_day'] = train['hour']*60 + train['minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['holidays'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ForecastId</th>\n",
       "      <th>Value</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week_day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>minutes_in_day</th>\n",
       "      <th>holidays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>91600</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>136500</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>335400</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>379000</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>344100</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  ForecastId   Value  year  month  day  week_day  hour  minute  \\\n",
       "0 2015-01-01           0   91600  2015      1    1         3     0       0   \n",
       "1 2015-01-02           0  136500  2015      1    2         4     0       0   \n",
       "2 2015-01-03           0  335400  2015      1    3         5     0       0   \n",
       "3 2015-01-04           0  379000  2015      1    4         6     0       0   \n",
       "4 2015-01-05           0  344100  2015      1    5         0     0       0   \n",
       "\n",
       "   minutes_in_day  holidays  \n",
       "0               0         0  \n",
       "1               0         0  \n",
       "2               0         0  \n",
       "3               0         0  \n",
       "4               0         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def lin_reg(x):\n",
    "    return np.sum( (X_valid_v['Value'] - np.dot(X_valid_stack,x) )**2 ) \n",
    "\n",
    "\n",
    "def constr_sum(x):\n",
    "    return np.sum(x)-1\n",
    "\n",
    "result = minimize(lin_reg, [0,0,0,0], method='SLSQP',\n",
    "                   bounds=((0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0)), \n",
    "                   constraints=({'type': 'eq', 'fun': lambda x: np.sum(x)-1}), \n",
    "                   tol=None, callback=None, options={'maxiter':100, 'disp':True})\n",
    "x = result['x']\n",
    "y_lr_minimize = np.sum(X_valid_stack*x, axis=1)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, fmin_slsqp\n",
    "\n",
    "def lin_reg(x):\n",
    "    # featurex - matrix of X \n",
    "    # x == b in linear regression \n",
    "    return np.sum( (y_true - np.dot(features,x) )**2 ) \n",
    "\n",
    "def constr_sum(x):\n",
    "    return np.sum(x)-1\n",
    "\n",
    "x0 = [0,0,0,0]\n",
    "\n",
    "result = minimize(lin_reg, x0 , method='SLSQP',\n",
    "                   bounds=((0.0,1.0), (0.0,1.0), (0.0,1.0), (0.0,1.0)), \n",
    "                   constraints=({'type': 'eq', 'fun': lambda x: np.sum(x)-1}), \n",
    "                   tol=None, callback=None, options={'maxiter':100, 'disp':True})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_fill(X):\n",
    "    \n",
    "    values_dict = dict(X[['Timestamp', 'Value']].values)\n",
    "    \n",
    "    for i in np.where(X['Value'].isnull())[0]:\n",
    "        val = []\n",
    "        for j in [-365, -14, -7, 7, 14, 365]:\n",
    "            ind = X['Timestamp']==(X['Timestamp'][i]+timedelta(days=j))\n",
    "            value = list(X['Value'][ind])\n",
    "            if (value!=[]):\n",
    "                val.append(value[0])\n",
    "        \n",
    "        X['Value'][i] = np.median(val)\n",
    "        # X['Value'] = X['Value'].interpolate()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "def validate_stack(X_stack, y, model):\n",
    "    y_cros_val_pred = cross_val_predict(model, X_stack, y=y, cv=5, n_jobs=-1)\n",
    "    return y_cros_val_pred\n",
    "\n",
    "def cv_lr(X_stack, y, model):\n",
    "    \n",
    "    # cros val lasso on X_stack \n",
    "    \n",
    "    y_cros_val_pred = pd.DataFrame({})\n",
    "    coefs = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_stack):\n",
    "        X_train, X_test = X_stack.iloc[train_index], X_stack.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_temp = model \n",
    "        model_temp.fit(X_train,y_train)\n",
    "        y_hat = model_temp.predict(X_test)\n",
    "        coefs.append(model_temp.coef_)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'id': test_index, 'Value': y_hat})\n",
    "        y_cros_val_pred = pd.concat([y_cros_val_pred, temp_df], axis=0)\n",
    "      \n",
    "    y_cros_val_pred = y_cros_val_pred.sort_values(by='id')\n",
    "    \n",
    "    #divide by sum - due to overfit of model, with dividing - coefs ~ weights of models\n",
    "    return list(y_cros_val_pred['Value']) / (np.sum(np.mean(coefs,axis=0)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_avg(x):       \n",
    "    \n",
    "    if (len(x)==1):\n",
    "        a = x\n",
    "    \n",
    "    if (len(x)==2):\n",
    "        a = (np.array([0.25, 0.75])*x).mean()\n",
    "        \n",
    "    if (len(x)==3):\n",
    "        a = (np.array([0.15, 0.25, 0.6])*x).mean()\n",
    "        \n",
    "    if (len(x)==4):\n",
    "        a = (np.array([0.1, 0.2, 0.3, 0.4])*x).mean()\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\pandas\\core\\groupby.py:707: FutureWarning: pd.rolling_sum is deprecated for DataFrame and will be removed in a future version, replace with \n",
      "\tDataFrame.rolling(min_periods=1,center=False,window=7).sum()\n",
      "  return func(g, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(287400, 13)\n",
      "(231466, 3)\n"
     ]
    }
   ],
   "source": [
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=0.0].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "print(train.shape)\n",
    "\n",
    "\n",
    "\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "#max_values = train[['Value', 'ForecastId']].groupby('ForecastId').agg('max').reset_index()\n",
    "#max_values.columns = ['ForecastId', 'max_value']\n",
    "#train = pd.merge(train, max_values, on = 'ForecastId', how='left')\n",
    "#train['Value'] = train['Value'] / train['max_value']\n",
    "#train = train.drop('max_value',axis=1)\n",
    "\n",
    "train['year_month_weekday'] = train['year'].astype('str') + '_' + train['month'].astype('str') + '_' + train['week_day'].astype('str') \n",
    "features = train[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "\n",
    "features = features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 Best model is rf_log  na value: 0 0\n",
      "loss: 5237.78686765\n",
      "val score: 5237.78686765\n",
      "{'median_3': 5253.137434955617, 'rf_mean_log': 5885.300937891277, 'lgb_log': 5635.036576788364, 'rf': 5746.5549040879, 'lgb_mean_log': 5977.796566981566, 'lgb_mean': 6252.283868561117, 'lgb': 6988.106515681309, 'rf_mean': 5725.327550984079, 'median_all': 5551.474339353127, 'mean_all': 5982.16848620889, 'rf_log': 5237.78686765294, 'mean_3': 5597.7689351426725}\n",
      "-------------------------------\n",
      "17 Best model is lgb_log  na value: 0 0\n",
      "loss: 1814.80309118\n",
      "val score: 3526.29497942\n",
      "{'median_3': 2220.104036878113, 'rf_mean_log': 4810.539129433211, 'lgb_log': 1814.8030911819146, 'rf': 2404.8755011646463, 'lgb_mean_log': 6254.80345443592, 'lgb_mean': 3061.053234247328, 'lgb': 2329.4346640305407, 'rf_mean': 3785.1489041541136, 'median_all': 2523.8865158706662, 'mean_all': 3181.1969923495635, 'rf_log': 3105.6674570840723, 'mean_3': 2368.7518502493563}\n",
      "-------------------------------\n",
      "18 Best model is lgb_log  na value: 0 0\n",
      "loss: 1713.92502046\n",
      "val score: 2922.17165977\n",
      "{'median_3': 3941.283667854496, 'rf_mean_log': 5313.758700581597, 'lgb_log': 1713.9250204646921, 'rf': 3529.170265240488, 'lgb_mean_log': 7445.625938821842, 'lgb_mean': 4996.050721327122, 'lgb': 4996.050721327122, 'rf_mean': 5281.736434588364, 'median_all': 5753.5101879206895, 'mean_all': 5258.079421466867, 'rf_log': 4412.864238907364, 'mean_3': 3842.15104431807}\n",
      "-------------------------------\n",
      "19 Best model is mean_all  na value: 0 0\n",
      "loss: 2758.90031394\n",
      "val score: 2881.35382331\n",
      "{'median_3': 2990.370623331934, 'rf_mean_log': 3268.5385677858144, 'lgb_log': 2784.941654655848, 'rf': 2885.778603748392, 'lgb_mean_log': 3049.112298972368, 'lgb_mean': 3044.4505901429056, 'lgb': 2942.952990137325, 'rf_mean': 3223.5303109035585, 'median_all': 2843.428554027394, 'mean_all': 2758.900313935888, 'rf_log': 2865.299637002641, 'mean_3': 2888.9000063079275}\n",
      "-------------------------------\n",
      "21 Best model is mean_3  na value: 0 0\n",
      "loss: 4454.32292511\n",
      "val score: 3195.94764367\n",
      "{'median_3': 4946.651087021416, 'rf_mean_log': 7674.140628770169, 'lgb_log': 9051.490029716353, 'rf': 5182.328556288384, 'lgb_mean_log': 9210.553545914534, 'lgb_mean': 4631.879412839912, 'lgb': 4591.537333028161, 'rf_mean': 5195.847500497854, 'median_all': 5006.993782356904, 'mean_all': 4678.0457595403805, 'rf_log': 6548.834196931781, 'mean_3': 4454.322925106825}\n",
      "-------------------------------\n",
      "22 Best model is mean_all  na value: 0 0\n",
      "loss: 2911.92827751\n",
      "val score: 3148.61108264\n",
      "{'median_3': 2912.7592225542107, 'rf_mean_log': 3698.144627433165, 'lgb_log': 4392.456627505727, 'rf': 3224.009956365144, 'lgb_mean_log': 3749.893054314182, 'lgb_mean': 2950.2770981501194, 'lgb': 2952.0098268597435, 'rf_mean': 3257.5103164425745, 'median_all': 3025.428850355632, 'mean_all': 2911.928277508678, 'rf_log': 4593.525861009994, 'mean_3': 2956.6571547709104}\n",
      "-------------------------------\n",
      "23 Best model is lgb_log  na value: 0 0\n",
      "loss: 8815.8898845\n",
      "val score: 3958.22234005\n",
      "{'median_3': 8857.343238609797, 'rf_mean_log': 9336.937815511637, 'lgb_log': 8815.889884496208, 'rf': 11016.383687168382, 'lgb_mean_log': 8934.26905069429, 'lgb_mean': 11295.295778074695, 'lgb': 10331.088259934886, 'rf_mean': 11303.626085962738, 'median_all': 9082.549742843785, 'mean_all': 11563.944147865117, 'rf_log': 9009.196711445333, 'mean_3': 10779.9860732026}\n",
      "-------------------------------\n",
      "24 Best model is mean_3  na value: 0 0\n",
      "loss: 1809.6751904\n",
      "val score: 3689.65394634\n",
      "{'median_3': 1826.6077060913121, 'rf_mean_log': 3911.92904254665, 'lgb_log': 2205.749867245953, 'rf': 2056.6961514918275, 'lgb_mean_log': 4263.2883368481935, 'lgb_mean': 1961.975415347937, 'lgb': 1913.8938355263156, 'rf_mean': 2426.802243177427, 'median_all': 1871.6926365646384, 'mean_all': 1911.683011061096, 'rf_log': 3191.2847517905975, 'mean_3': 1809.675190401841}\n",
      "-------------------------------\n",
      "25 Best model is mean_all  na value: 0 0\n",
      "loss: 2966.19193939\n",
      "val score: 3609.2692789\n",
      "{'median_3': 3073.7642729790223, 'rf_mean_log': 4077.2487160697656, 'lgb_log': 3109.5560751209528, 'rf': 3082.5160086328697, 'lgb_mean_log': 4116.594085329119, 'lgb_mean': 3173.165282705993, 'lgb': 3229.4895155449894, 'rf_mean': 3120.1456095867075, 'median_all': 3028.95375744471, 'mean_all': 2966.191939390983, 'rf_log': 3563.425301430152, 'mean_3': 3116.007300647963}\n",
      "-------------------------------\n",
      "26 Best model is median_3  na value: 0 0\n",
      "loss: 3471.42579978\n",
      "val score: 3595.48493099\n",
      "{'median_3': 3471.425799775804, 'rf_mean_log': 6412.960531170487, 'lgb_log': 3556.1866179202984, 'rf': 4065.3316880518482, 'lgb_mean_log': 6257.850174497994, 'lgb_mean': 5698.921094337716, 'lgb': 4024.938579497591, 'rf_mean': 5878.327217881974, 'median_all': 3552.2117789083386, 'mean_all': 3880.1740084668863, 'rf_log': 4920.899200719105, 'mean_3': 3720.2167945733095}\n",
      "-------------------------------\n",
      "27 Best model is mean_all  na value: 0 0\n",
      "loss: 2171.50282954\n",
      "val score: 3466.03201268\n",
      "{'median_3': 2279.134671481055, 'rf_mean_log': 4637.081913155673, 'lgb_log': 3760.8957852663257, 'rf': 2781.1961077137994, 'lgb_mean_log': 4099.0062073642475, 'lgb_mean': 2623.337997471082, 'lgb': 2484.7361946592246, 'rf_mean': 3267.0715025927284, 'median_all': 2216.3005944403276, 'mean_all': 2171.5028295419643, 'rf_log': 6335.010364367114, 'mean_3': 2209.9696491548166}\n",
      "-------------------------------\n",
      "28 Best model is median_all  na value: 0 0\n",
      "loss: 3593.24267657\n",
      "val score: 3476.63290134\n",
      "{'median_3': 3645.0008985802433, 'rf_mean_log': 4885.33180612796, 'lgb_log': 3850.8525361643738, 'rf': 4347.175356091801, 'lgb_mean_log': 4001.606456974875, 'lgb_mean': 4614.348356639322, 'lgb': 4299.551773401417, 'rf_mean': 4822.421539668984, 'median_all': 3593.242676571018, 'mean_all': 3681.6430040535947, 'rf_log': 4118.676332597116, 'mean_3': 3845.6119331456307}\n",
      "-------------------------------\n",
      "29 Best model is lgb  na value: 0 0\n",
      "loss: 3200.45854591\n",
      "val score: 3455.38872015\n",
      "{'median_3': 3332.560610451077, 'rf_mean_log': 3889.385672901514, 'lgb_log': 3386.0990496622035, 'rf': 3272.510606011275, 'lgb_mean_log': 3938.3691638350456, 'lgb_mean': 3802.6279957466018, 'lgb': 3200.4585459132513, 'rf_mean': 3543.9858202902296, 'median_all': 3348.401429537332, 'mean_all': 3463.5472579836537, 'rf_log': 3460.8429043931365, 'mean_3': 3635.0043465662125}\n",
      "-------------------------------\n",
      "30 Best model is median_all  na value: 0 0\n",
      "loss: 3388.0831928\n",
      "val score: 3450.58118248\n",
      "{'median_3': 3927.150084317032, 'rf_mean_log': 4774.332449519405, 'lgb_log': 3989.9930525470913, 'rf': 4630.855784197552, 'lgb_mean_log': 4041.244659566487, 'lgb_mean': 4281.9027780604965, 'lgb': 4292.611390488801, 'rf_mean': 4592.30501391937, 'median_all': 3388.0831928049465, 'mean_all': 3627.8612917904657, 'rf_log': 4386.1680888523315, 'mean_3': 3952.804946599214}\n",
      "-------------------------------\n",
      "31 Best model is rf_mean_log  na value: 0 0\n",
      "loss: 5482.39881193\n",
      "val score: 3586.03569111\n",
      "{'median_3': 10001.771892194349, 'rf_mean_log': 5482.398811925254, 'lgb_log': 6723.055691915011, 'rf': 10963.022769653551, 'lgb_mean_log': 5499.506107306886, 'lgb_mean': 8672.427001668153, 'lgb': 9071.439734762856, 'rf_mean': 8646.867404209628, 'median_all': 9882.635456495384, 'mean_all': 9346.668043324766, 'rf_log': 8873.49573959238, 'mean_3': 9727.080530057778}\n",
      "-------------------------------\n",
      "32 Best model is mean_all  na value: 0 0\n",
      "loss: 2074.59067206\n",
      "val score: 3491.57037742\n",
      "{'median_3': 2225.1484484425246, 'rf_mean_log': 4417.932456224947, 'lgb_log': 2718.8223297528066, 'rf': 2310.779517659926, 'lgb_mean_log': 4400.887015841531, 'lgb_mean': 4631.070081165799, 'lgb': 2521.301546522504, 'rf_mean': 4817.340321582761, 'median_all': 2257.994515791758, 'mean_all': 2074.5906720635967, 'rf_log': 2822.215659467118, 'mean_3': 2144.1711264608825}\n",
      "-------------------------------\n",
      "33 Best model is lgb_mean_log  na value: 0 0\n",
      "loss: 7701.61301296\n",
      "val score: 3739.21994422\n",
      "{'median_3': 18769.98185431608, 'rf_mean_log': 8006.685296519854, 'lgb_log': 10708.315554650644, 'rf': 15225.90271575462, 'lgb_mean_log': 7701.613012962346, 'lgb_mean': 8897.83814982353, 'lgb': 12911.67882816656, 'rf_mean': 10669.676711214706, 'median_all': 18513.35003888361, 'mean_all': 26225.594878061824, 'rf_log': 11238.633799589825, 'mean_3': 27305.944563084027}\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Best model is lgb_mean  na value: 0 0\n",
      "loss: 5785.7774531\n",
      "val score: 3852.9175836\n",
      "{'median_3': 5936.33732947499, 'rf_mean_log': 6216.01132846321, 'lgb_log': 8174.9714485530285, 'rf': 6350.891169051941, 'lgb_mean_log': 6457.610302032234, 'lgb_mean': 5785.777453096664, 'lgb': 6618.474026642426, 'rf_mean': 5978.637164050333, 'median_all': 5970.959073997519, 'mean_all': 6087.041575621567, 'rf_log': 7749.384880468684, 'mean_3': 6175.830232878602}\n",
      "-------------------------------\n",
      "35 Best model is mean_3  na value: 0 0\n",
      "loss: 2306.13881509\n",
      "val score: 3771.50817473\n",
      "{'median_3': 2330.5214442254246, 'rf_mean_log': 5747.994901359805, 'lgb_log': 2533.5907310475723, 'rf': 2694.292093248559, 'lgb_mean_log': 5749.431473232418, 'lgb_mean': 3514.907107118067, 'lgb': 3708.513307042499, 'rf_mean': 3525.204387134311, 'median_all': 2462.872044761683, 'mean_all': 2427.128316708615, 'rf_log': 4507.303254713963, 'mean_3': 2306.138815088703}\n",
      "-------------------------------\n",
      "36 Best model is mean_3  na value: 0 0\n",
      "loss: 3551.07421702\n",
      "val score: 3760.48647685\n",
      "{'median_3': 3802.858671380722, 'rf_mean_log': 4641.871752566951, 'lgb_log': 4280.804679828792, 'rf': 4210.840426089765, 'lgb_mean_log': 4626.333395329463, 'lgb_mean': 3667.4812409699894, 'lgb': 3959.7288818113607, 'rf_mean': 3908.2133670093317, 'median_all': 3660.797357647935, 'mean_all': 3643.4607167638287, 'rf_log': 4536.056636391413, 'mean_3': 3551.0742170193353}\n",
      "-------------------------------\n",
      "38 Best model is median_all  na value: 0 0\n",
      "loss: 2473.14842978\n",
      "val score: 3699.18466508\n",
      "{'median_3': 2702.5036118926314, 'rf_mean_log': 4490.254124777783, 'lgb_log': 3487.41395606132, 'rf': 3494.9199300905843, 'lgb_mean_log': 3977.588241647311, 'lgb_mean': 2500.0163878976027, 'lgb': 2495.9161359172895, 'rf_mean': 3870.4210604415366, 'median_all': 2473.148429777203, 'mean_all': 2602.436647041144, 'rf_log': 3774.90582267181, 'mean_3': 2625.1394063822777}\n",
      "-------------------------------\n",
      "40 Best model is median_all  na value: 0 0\n",
      "loss: 5092.65901483\n",
      "val score: 3762.52440825\n",
      "{'median_3': 6482.544237207078, 'rf_mean_log': 7494.721155902333, 'lgb_log': 5712.369817125109, 'rf': 5557.894792438198, 'lgb_mean_log': 7329.592577324338, 'lgb_mean': 6855.736407262652, 'lgb': 5779.899632957269, 'rf_mean': 7028.43390564961, 'median_all': 5092.659014825443, 'mean_all': 5713.696454191434, 'rf_log': 5753.374920598218, 'mean_3': 6068.050289221378}\n",
      "-------------------------------\n",
      "41 Best model is mean_3  na value: 0 0\n",
      "loss: 1857.94748497\n",
      "val score: 3679.71671593\n",
      "{'median_3': 1904.9334416808383, 'rf_mean_log': 4601.8246831824, 'lgb_log': 1901.0218276556343, 'rf': 2473.220685875357, 'lgb_mean_log': 4313.484826636666, 'lgb_mean': 2293.8581350613467, 'lgb': 1890.5888720522996, 'rf_mean': 3526.342505396269, 'median_all': 1953.2918932051036, 'mean_all': 2064.3611506377383, 'rf_log': 2952.3118254156216, 'mean_3': 1857.9474849695755}\n",
      "-------------------------------\n",
      "43 Best model is rf_log  na value: 0 0\n",
      "loss: 4035.26942049\n",
      "val score: 3694.53141196\n",
      "{'median_3': 4885.923600209314, 'rf_mean_log': 5165.999634454262, 'lgb_log': 4503.536969739314, 'rf': 5057.8957769765, 'lgb_mean_log': 5315.412931012754, 'lgb_mean': 5697.895621252615, 'lgb': 4609.750322365458, 'rf_mean': 5561.484588094709, 'median_all': 4725.798011512297, 'mean_all': 5805.702574069922, 'rf_log': 4035.2694204898075, 'mean_3': 5765.742194313623}\n",
      "-------------------------------\n",
      "44 Best model is lgb_mean_log  na value: 0 0\n",
      "loss: 5990.54908177\n",
      "val score: 3786.37211875\n",
      "{'median_3': 6009.124268570862, 'rf_mean_log': 6165.592167773003, 'lgb_log': 6273.181803559492, 'rf': 6422.970788006658, 'lgb_mean_log': 5990.54908176525, 'lgb_mean': 6154.980207837598, 'lgb': 6200.658693064326, 'rf_mean': 6095.003044847742, 'median_all': 6172.766041852623, 'mean_all': 6020.7161511832755, 'rf_log': 6150.938490844571, 'mean_3': 6231.7431981222535}\n",
      "-------------------------------\n",
      "45 Best model is rf_log  na value: 0 0\n",
      "loss: 5259.25304307\n",
      "val score: 3843.02138507\n",
      "{'median_3': 6902.364273204904, 'rf_mean_log': 6772.1302408355095, 'lgb_log': 8473.95401793235, 'rf': 7282.9027159314155, 'lgb_mean_log': 6194.42337513129, 'lgb_mean': 8219.454341714141, 'lgb': 8196.183087561774, 'rf_mean': 9246.033835756056, 'median_all': 5416.423428682623, 'mean_all': 7210.454136898969, 'rf_log': 5259.253043065905, 'mean_3': 8249.309377866733}\n",
      "-------------------------------\n",
      "47 Best model is median_all  na value: 0 0\n",
      "loss: 5636.42551536\n",
      "val score: 3909.44376027\n",
      "{'median_3': 6045.313107740179, 'rf_mean_log': 6632.512212320485, 'lgb_log': 5935.169224427077, 'rf': 5941.084619623065, 'lgb_mean_log': 6035.088674993947, 'lgb_mean': 5648.801620193404, 'lgb': 5644.912083048324, 'rf_mean': 6214.639120553786, 'median_all': 5636.425515363672, 'mean_all': 5742.352892148692, 'rf_log': 6108.532756071572, 'mean_3': 5856.994684299236}\n",
      "-------------------------------\n",
      "48 Best model is lgb_mean  na value: 0 0\n",
      "loss: 8377.81783338\n",
      "val score: 4069.02854859\n",
      "{'median_3': 8561.621840732376, 'rf_mean_log': 8834.429602187854, 'lgb_log': 8561.621708951248, 'rf': 8776.185596339928, 'lgb_mean_log': 8539.796560375047, 'lgb_mean': 8377.817833377921, 'lgb': 9002.10886404752, 'rf_mean': 8754.950620032836, 'median_all': 8561.621840732376, 'mean_all': 8561.621840732376, 'rf_log': 9301.20161933803, 'mean_3': 8561.621840732376}\n",
      "-------------------------------\n",
      "49 Best model is mean_all  na value: 0 0\n",
      "loss: 3646.87766489\n",
      "val score: 4054.47162157\n",
      "{'median_3': 4091.1782119597397, 'rf_mean_log': 7739.968769461356, 'lgb_log': 5648.295872493334, 'rf': 5311.293723307499, 'lgb_mean_log': 7672.277489186547, 'lgb_mean': 6816.004204136855, 'lgb': 5070.208389586721, 'rf_mean': 8595.35011916849, 'median_all': 4056.500042290451, 'mean_all': 3646.877664893398, 'rf_log': 5750.788879575765, 'mean_3': 3803.897669192011}\n",
      "-------------------------------\n",
      "50 Best model is mean_all  na value: 0 0\n",
      "loss: 1955.57551534\n",
      "val score: 3984.50841803\n",
      "{'median_3': 2190.191763596353, 'rf_mean_log': 3727.6434592218443, 'lgb_log': 2503.6869707668934, 'rf': 2152.420342254883, 'lgb_mean_log': 4041.1225175896943, 'lgb_mean': 3454.258089690218, 'lgb': 3029.4128956508575, 'rf_mean': 3529.1500972290114, 'median_all': 2152.5463690663314, 'mean_all': 1955.575515336596, 'rf_log': 2543.2106951953406, 'mean_3': 2000.6449192377363}\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-454a4ee0d18b>\u001b[0m in \u001b[0;36mfeature_preprocessing\u001b[1;34m(train, test, cat_cols, cat_type)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcat_type\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'ohe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2017\u001b[0m         \"\"\"\n\u001b[0;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[1;32m-> 2019\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m     \"\"\"\n\u001b[1;32m-> 1809\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    429\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 431\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['week_day','hour','minute','month']\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[11:]:\n",
    " \n",
    "    rf_stack = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "    lr_stack = Lasso(alpha=1, fit_intercept=False, max_iter=3000, tol=0.0001, positive=True, random_state=424142)\n",
    "\n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 180\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    \n",
    "    # train on last 1.5 year \n",
    "    na_values_before = np.sum(X_train['Value'].isnull())\n",
    "    X_train = nan_fill(X_train)\n",
    "    na_values_after = np.sum(X_train['Value'].isnull())\n",
    "    X_train = X_train.dropna(subset=['Value']).reset_index(drop=True)\n",
    "    \n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "    \n",
    "    # drop outliers in train data set \n",
    "    #up_border = X_train['Value'].quantile(0.985)\n",
    "    #low_border = X_train['Value'].quantile(0.015)\n",
    "    #X_train = X_train[(X_train['Value']<=up_border) & (X_train['Value']>=low_border)].reset_index(drop=True)\n",
    "    \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = X_train \n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe')\n",
    "    X_train_meanenc, X_test_meanenc = feature_preprocessing(X_train, X_test, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    X_train_v_meanenc, X_valid_v_meanenc = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_mean = train_rf(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    y_rf_mean_log = np.exp(train_rf(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc)) - 1\n",
    "    \n",
    "        # LightGBM\n",
    "    y_lgb, lgb_opt = validate_lgb(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe, X_valid_v['Value'])\n",
    "    y_lgb_mean, lgb_mean_opt = validate_lgb(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc, X_valid_v['Value'])\n",
    "    y_lgb_log, lgb_opt_log = validate_lgb(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe, X_valid_v['Value_log'])\n",
    "    y_lgb_log = np.exp(y_lgb_log) -1\n",
    "    y_lgb_mean_log, lgb_mean_opt_log = validate_lgb(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc, X_valid_v['Value_log'])\n",
    "    y_lgb_mean_log = np.exp(y_lgb_mean_log) -1\n",
    "    \n",
    "    # stack predictions and make predictions \n",
    "    #X_valid_stack = combine_y_hats([y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                                    #y_hat_grby_median, y_hat_grby3_median, \n",
    "                                    #y_rf, y_rf_mean, \n",
    "                                    #y_rf_log, y_rf_mean_log, \n",
    "                                    #y_lgb, y_lgb_mean, \n",
    "                                   # y_lgb_log, y_lgb_mean_log\n",
    "                                   #])\n",
    "    #y_rf_hat = validate_stack(X_valid_stack, X_valid_v['Value'], rf_stack)\n",
    "    #y_lr_hat = cv_lr(X_valid_stack, X_valid_v['Value'], lr_stack)\n",
    " \n",
    "    # calculate scores and pick top model \n",
    "    iid = X_valid_v.reset_index()['index'] \n",
    "    T = np.max(iid)\n",
    "    index_mult = (3*T -2*iid +1) / 2 / T**2\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean = calc_score(y_rf_mean, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean_log = calc_score(y_rf_mean_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_lgb = calc_score(y_lgb, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_mean = calc_score(y_lgb_mean, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_log = calc_score(y_lgb_log, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_mean_log = calc_score(y_lgb_mean_log, X_valid_v['Value'], index_mult)\n",
    "\n",
    "    #score_lr_stack = calc_score(y_lr_hat, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    \n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, y_rf_mean, \n",
    "                  y_rf_log, y_rf_mean_log, \n",
    "                  y_lgb, y_lgb_mean, \n",
    "                  y_lgb_log, y_lgb_mean_log, \n",
    "                  #y_lr_hat\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, score_rf_mean, \n",
    "                  score_rf_log, score_rf_mean_log, \n",
    "                score_lgb, score_lgb_mean, \n",
    "                  score_lgb_log, score_lgb_mean_log, \n",
    "                  #score_lr_stack\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_mean', \n",
    "                    'rf_log', 'rf_mean_log', \n",
    "                    'lgb', 'lgb_mean', \n",
    "                    'lgb_log', 'lgb_mean_log', \n",
    "                    'lr_stack'\n",
    "                   ]\n",
    "    # plot figures and seve to folder \n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    ax.plot(y_hats[np.argmin(all_scores)])\n",
    "    ax.plot(X_valid_v['Value'].reset_index(drop=True))\n",
    "    fig.savefig('C:/Users/denis/Machine_Learning_Competitions/idao/lr_stack/'+str(i)+'.jpg')   # save the figure to file\n",
    "    # plt.show()\n",
    "    plt.close(fig)    # close the figure\n",
    "    \n",
    "    # calc R2 and save later \n",
    "    #r2 = r2_score(y_hats[np.argmin(all_scores)], X_valid_v['Value'] ) \n",
    "    #losses.append( r2 )\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)], ' na value:', na_values_before, na_values_after)\n",
    "    print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
