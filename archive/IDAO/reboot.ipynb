{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T13:32:10.884618Z",
     "start_time": "2018-02-11T13:31:56.200074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46996828, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom metric is implemented here\n",
    "from scorer import scorer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Use custom dtypes for efficiency\n",
    "dtypes = {'id1': np.int16, 'id2': np.int16, 'id3': np.int16, 'user_id': np.int32, 'date': np.int16}\n",
    "\n",
    "train = pd.read_csv('train.csv.zip', dtype=dtypes)\n",
    "# train_const = train.copy()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ~~Посмотреть что он открывал 4 недели назад и больше но не открывал за 3 недели и есть ли тут пересечения~~\n",
    "* Предсказывать юзеров которые каждую неделю открывают минимум 5 новых категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-11T12:54:20.498Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# drop 'inactive' and 'overactive' users \n",
    "\n",
    "# user_count = np.unique(train_const.user_id[train_const.date>(np.max(train_const.date)-7)], return_counts=True)\n",
    "user_count = np.unique(train.user_id[train.date>(np.max(train.date)-7)], return_counts=True)\n",
    "user_count = pd.DataFrame({\n",
    "    'user_id':user_count[0], 'counts':user_count[1]\n",
    "})\n",
    "\n",
    "#print(user_count.shape)\n",
    "treshold_count_low = 8\n",
    "treshold_count_high = 50\n",
    "user_count = user_count[(user_count.counts>=treshold_count_low) & (user_count.counts<treshold_count_high)]\n",
    "\n",
    "\n",
    "# user_count = user_count[:10000]\n",
    "# user_count = user_count[0:5500]\n",
    "print(user_count.shape)\n",
    "\n",
    "train = pd.merge(train, user_count, how='left', on ='user_id')\n",
    "train = train.dropna()\n",
    "# print(train.shape, len(pd.unique(train.user_id)))\n",
    "\n",
    "del user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T19:03:10.478237Z",
     "start_time": "2018-02-09T19:03:10.457697Z"
    }
   },
   "source": [
    "Забавный факт - увеличивая скор на валидации с помощью изменения трешхолда treshold_count_low и второго, на сабмите скор падает\n",
    "### Пример:\n",
    "    \n",
    "    валидация - паблик\n",
    "    2600 - 1900\n",
    "    3600 - 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T12:54:57.959643Z",
     "start_time": "2018-02-11T12:54:57.954229Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Use token 1260389131217015787.\n"
     ]
    }
   ],
   "source": [
    "import telepyth # push notif in telegram\n",
    "\n",
    "%telepyth -t 1260389131217015787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T12:56:23.227803Z",
     "start_time": "2018-02-11T12:54:57.960681Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "date_validation_start = train.date.max() - 6\n",
    "\n",
    "def calculate_target(data, date_test_start):\n",
    "    '''\n",
    "        This function returns a dictionary of type {user: items_list}\n",
    "        Such that user viewed an item in testing period, \n",
    "        but did not view it within the last 3 weeks of train period.\n",
    "    '''\n",
    "    \n",
    "    test_mask = (data.date >= date_test_start) & (data.date < date_test_start + 7)\n",
    "    last_3weeks_mask = (data.date >= date_test_start - 21 + 1) & (data.date < date_test_start)\n",
    "    \n",
    "    # Items that used viewed during test period\n",
    "    items_test = data[test_mask].groupby('user_id').id3.apply(set)\n",
    "    \n",
    "    # Items, that user viewd in last 3 weeks\n",
    "    user_last_3weeks = data[last_3weeks_mask].groupby('user_id').id3.apply(set)\n",
    "    \n",
    "    # Get table, where for each `user_id` we have both items from test period and 3 weeks\n",
    "    joined = items_test.reset_index().merge(user_last_3weeks.reset_index(), on=['user_id'], how='left')\n",
    "    joined.set_index('user_id', inplace=True)\n",
    "    \n",
    "    # Remove the items, which the user viewed during last 3 weeks \n",
    "    target = {}\n",
    "    for user_id, (id3_x, id3_y) in joined.iterrows():   \n",
    "        items = id3_x if id3_y is np.nan else id3_x - id3_y\n",
    "        if items != set(): target.update({user_id: items})\n",
    "\n",
    "    return target\n",
    "\n",
    "y_val_dict = calculate_target(train, date_validation_start)\n",
    "# np.save('y_val_dict', y_val_dict)\n",
    "# y_val_dict = np.load('y_val_dict.npy').item()\n",
    "\n",
    "y_val_dict1 = calculate_target(train, date_validation_start-7)\n",
    "y_val_dict2 = calculate_target(train, date_validation_start-7*2)\n",
    "\n",
    "\n",
    "ind_list = []\n",
    "for k, v in y_val_dict.items():\n",
    "    try:\n",
    "        if (len(v) > 4) and (len(y_val_dict1[k]) > 4) and (len(y_val_dict2[k]) > 4): \n",
    "            ind_list.append(k)\n",
    "    except:\n",
    "        continue\n",
    "len(ind_list)\n",
    "\n",
    "np.save(\"ind_list\", ind_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T18:52:52.569619Z",
     "start_time": "2018-02-09T18:52:41.071000Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "num = 53979\n",
    "\n",
    "mask_train = (train.date < date_validation_start - 7) & (train.date > date_validation_start - 7*4)\n",
    "mask_test = (train.date < date_validation_start) & (train.date >= date_validation_start - 7*3)\n",
    "\n",
    "# users = train.loc[mask_test].user_id.value_counts().index[:num]\n",
    "# mask_users = train.user_id.isin(users)\n",
    "\n",
    "# mask_train = mask_train & mask_users\n",
    "# mask_test = mask_test & mask_users\n",
    "# users_mask = train.user_id < 10000\n",
    "# mask_train = mask_train & users_mask\n",
    "\n",
    "\n",
    "def get_feats(data):\n",
    "    '''\n",
    "        Builds sparse matrix using users' history.\n",
    "    '''\n",
    "    return scipy.sparse.coo_matrix(([1] * data.shape[0], (data.user_id, data.id3)), \n",
    "                                    shape =[data.user_id.max()+1, data.id3.max()+1]).tocsr()\n",
    "\n",
    "def get_target_matrix(X, target_dict):\n",
    "    '''\n",
    "        Builds sparse matrix using dictionary.\n",
    "    '''\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "\n",
    "    ks = []\n",
    "    for k in tqdm(range(X.user_id.max()+1)):\n",
    "        d = target_dict.get(k, [])\n",
    "        for y in d:\n",
    "            indices.append(y)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return scipy.sparse.csr_matrix((data, indices, indptr), dtype=int, shape =[X.user_id.max()+1, X.id3.max()+1])\n",
    "\n",
    "X_train = get_feats(train.loc[mask_train])\n",
    "X_test = get_feats(train.loc[mask_test])\n",
    "\n",
    "y_train_dict = calculate_target(train, date_validation_start - 7)\n",
    "y_train = get_target_matrix(train.loc[mask_train], y_train_dict)\n",
    "y_test = get_target_matrix(train.loc[mask_test], y_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T18:52:52.569619Z",
     "start_time": "2018-02-09T18:52:41.071000Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dot = np.dot(X_test_nmf, H)\n",
    "# тут логика такая: раскладываем матрицу \"оценок\" на фичи юзеров и фичи айди3, потом раскладываем такую же матрицу на новых \n",
    "# данных и умножаем на фичи айдишек\n",
    "\n",
    "users = train.loc[mask_test].user_id.value_counts().index[:num]\n",
    "# users = pd.Int64Index(idx)\n",
    "ans_inds =  np.argsort(dot[users])\n",
    "# test_inds_dict =  {k: list(ans_inds[i, -5:]) for i,k in enumerate(users)}\n",
    "# scorer(y_val_dict, test_inds_dict, num_users=num/0.05)\n",
    "\n",
    "last_3weeks = train.loc[mask_test].loc[train.loc[mask_test].date >= train.loc[mask_test].date.max() - 21 + 1]\n",
    "y_not = last_3weeks.groupby('user_id').id3.apply(set)\n",
    "\n",
    "y_pred = {}\n",
    "num_users=num/0.05\n",
    "for u_idx, user in tqdm(enumerate(users)):\n",
    "    items_not = y_not.get(user, [])\n",
    "    items_pred = []\n",
    "    i = 1\n",
    "    while len(items_pred) < 5:\n",
    "        if not ans_inds[u_idx, -i] in items_not:\n",
    "            items_pred += [ans_inds[u_idx, -i]]\n",
    "    \n",
    "        i += 1\n",
    "    y_pred.update({user: items_pred})\n",
    "\n",
    "num_users = 53979/0.05\n",
    "score = scorer(y_val_dict, y_pred, num_users)\n",
    "%telepyth 'Very magic, wow!\\n' + str(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T18:55:49.370144Z",
     "start_time": "2018-02-09T18:53:18.084014Z"
    }
   },
   "outputs": [],
   "source": [
    "mask_sub = train.date >= date_validation_start\n",
    "X_sub = get_feats(train.loc[mask_sub])\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=5, init='random', random_state=0)\n",
    "x_train_nmf = nmf.fit_transform(X_test)\n",
    "y_test_nmf = nmf.components_\n",
    "x_sub_nmf = nmf.transform(X_sub)\n",
    "dot = np.dot(x_sub_nmf, y_test_nmf)\n",
    "\n",
    "users = train.loc[mask_test].user_id.value_counts().index[:num]\n",
    "# users = pd.Int64Index(idx)\n",
    "ans_inds =  np.argsort(dot[users])\n",
    "# test_inds_dict =  {k: list(ans_inds[i, -5:]) for i,k in enumerate(users)}\n",
    "# scorer(y_val_dict, test_inds_dict, num_users=num/0.05)\n",
    "\n",
    "last_3weeks = train.loc[mask_test].loc[train.loc[mask_test].date >= train.loc[mask_test].date.max() - 21 + 1]\n",
    "y_not = last_3weeks.groupby('user_id').id3.apply(set)\n",
    "\n",
    "y_pred = {}\n",
    "for u_idx, user in tqdm(enumerate(users)):\n",
    "    items_not = y_not.get(user, [])\n",
    "    items_pred = []\n",
    "    i = 1\n",
    "    while len(items_pred) < 5:\n",
    "        if not ans_inds[u_idx, -i] in items_not:\n",
    "            items_pred += [ans_inds[u_idx, -i]]\n",
    "    \n",
    "        i += 1\n",
    "    y_pred.update({user: items_pred})\n",
    "# num_users = 49130/0.05\n",
    "# score = scorer(y_val_dict, y_pred, num_users)\n",
    "# %telepyth 'Very magic, wow!\\n' #+ str(score)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T18:55:51.865971Z",
     "start_time": "2018-02-09T18:55:49.392033Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame.from_records(y_pred).T.reset_index()\n",
    "y_pred_df.columns = ['user_id', 'id3_1', 'id3_2', 'id3_3', 'id3_4', 'id3_5']\n",
    "\n",
    "y_pred_df.to_csv('y_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это попытка предсказания каких юзеров выбирать, я думаю на этом стоит остановиться, может быть кор метрикой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-09T18:55:51.877634Z",
     "start_time": "2018-02-09T18:55:51.868527Z"
    }
   },
   "outputs": [],
   "source": [
    "# # del Y_test, Y_train\n",
    "# Y_train = np.ndarray((1179572))\n",
    "# for key in tqdm(y_train_dict):\n",
    "#     Y_train[key] = len(y_train_dict[key])\n",
    "\n",
    "# Y_test = np.ndarray((1179572))\n",
    "# for key in tqdm(y_val_dict):\n",
    "#     Y_test[key] = len(y_val_dict[key])\n",
    "\n",
    "# Y_train = Y_train.astype('int32')\n",
    "# Y_test = Y_test.astype('int32')\n",
    "\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# verbose = 1\n",
    "# plot = 1\n",
    "# params = {\n",
    "# #     \"max_bin\": 1024,\n",
    "#     \"learning_rate\": 0.01,\n",
    "#     \"boosting_type\": \"goss\",\n",
    "#     \"objective\": \"regression\",\n",
    "#     'num_iterations':10000,\n",
    "# #     \"metric\": \"auc\",\n",
    "#     \"num_leaves\": 10000,\n",
    "#     \"verbose\": 1,\n",
    "# #     \"min_data\": 100,\n",
    "# #     \"boost_from_average\": True\n",
    "# }\n",
    "\n",
    "# d_train = lgb.Dataset(X_train_nmf, Y_train)\n",
    "# # d_valid = lgb.Dataset(X_test, label=y_test)\n",
    "# model = lgb.train(params, d_train)\n",
    "# # model.save_model('regressin')\n",
    "# # model = lgb.Booster(model_file='regressin')\n",
    "\n",
    "# y_pred = model.predict(X_test_nmf)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# # print(roc_auc_score(y_test.values, y_pred))\n",
    "# print(mean_squared_error(y_pred ,Y_test)**0.5)\n",
    "\n",
    "# df_fi = pd.DataFrame(model.feature_name(), columns=['feature'])\n",
    "# df_fi['importance'] = list(model.feature_importance('gain'))\n",
    "# df_fi.sort_values('importance', ascending=False, inplace=True)\n",
    "# # print(df_fi)\n",
    "# if plot:\n",
    "#     plt.figure()\n",
    "#     df_fi.head(10).plot(kind='barh', x='feature', y='importance')\n",
    "#     plt.title('LightGBM Feature Importance')\n",
    "#     plt.xlabel('relative importance')\n",
    "#     plt.show()\n",
    "\n",
    "# idx = (y_pred).argsort()[:53979]\n",
    "# idx # ids to test main task\n",
    "# %telepyth 'lgb!\\n' + str(mean_squared_error(y_pred ,Y_test)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
