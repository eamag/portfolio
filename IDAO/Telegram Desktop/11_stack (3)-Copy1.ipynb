{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T16:10:51.992606Z",
     "start_time": "2018-04-03T16:10:49.458122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eamag/.pyenv/versions/3.6.0/envs/general/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.optimize import minimize, fmin_slsqp\n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pylab\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import lightgbm as lgb \n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train and sub format already merged with weather and holidays (plus the day before holiday)\n",
    "train = pd.read_csv('train.csv')\n",
    "hol = pd.read_csv('hol.csv')\n",
    "hol['Timestamp'] = pd.to_datetime(hol['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv('gold_usd/gold.csv')\n",
    "usd = pd.read_csv('gold_usd/usd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>DATE</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>VOL</th>\n",
       "      <th>WAPRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1570.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1543.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1561.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>620.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1543.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1539.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>610.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1535.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1518.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>605.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1544.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1507.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1554.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1516.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>605.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1566.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1534.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1584.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1748.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1822.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2428.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1747.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1805.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2464.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1751.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1802.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5912</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2471.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1797.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2482.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1759.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1797.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2481.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5921</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1743.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5923</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1808.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5924</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2458.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1738.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1803.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2437.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1723.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1785.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5932 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TICKER        DATE  OPEN  HIGH  LOW    CLOSE  VOL  WAPRICE\n",
       "0          1  2012-04-03   NaN   NaN  NaN  1570.08  NaN      NaN\n",
       "1          2  2012-04-03   NaN   NaN  NaN    30.60  NaN      NaN\n",
       "2          3  2012-04-03   NaN   NaN  NaN  1543.66  NaN      NaN\n",
       "3          4  2012-04-03   NaN   NaN  NaN   618.97  NaN      NaN\n",
       "4          1  2012-04-04   NaN   NaN  NaN  1577.34  NaN      NaN\n",
       "5          2  2012-04-04   NaN   NaN  NaN    30.53  NaN      NaN\n",
       "6          3  2012-04-04   NaN   NaN  NaN  1561.56  NaN      NaN\n",
       "7          4  2012-04-04   NaN   NaN  NaN   620.67  NaN      NaN\n",
       "8          1  2012-04-05   NaN   NaN  NaN  1543.88  NaN      NaN\n",
       "9          2  2012-04-05   NaN   NaN  NaN    31.19  NaN      NaN\n",
       "10         3  2012-04-05   NaN   NaN  NaN  1539.38  NaN      NaN\n",
       "11         4  2012-04-05   NaN   NaN  NaN   610.27  NaN      NaN\n",
       "12         1  2012-04-06   NaN   NaN  NaN  1535.22  NaN      NaN\n",
       "13         2  2012-04-06   NaN   NaN  NaN    30.26  NaN      NaN\n",
       "14         3  2012-04-06   NaN   NaN  NaN  1518.66  NaN      NaN\n",
       "15         4  2012-04-06   NaN   NaN  NaN   605.57  NaN      NaN\n",
       "16         1  2012-04-07   NaN   NaN  NaN  1544.85  NaN      NaN\n",
       "17         2  2012-04-07   NaN   NaN  NaN    29.62  NaN      NaN\n",
       "18         3  2012-04-07   NaN   NaN  NaN  1507.91  NaN      NaN\n",
       "19         4  2012-04-07   NaN   NaN  NaN   601.46  NaN      NaN\n",
       "20         1  2012-04-10   NaN   NaN  NaN  1554.04  NaN      NaN\n",
       "21         2  2012-04-10   NaN   NaN  NaN    29.79  NaN      NaN\n",
       "22         3  2012-04-10   NaN   NaN  NaN  1516.88  NaN      NaN\n",
       "23         4  2012-04-10   NaN   NaN  NaN   605.04  NaN      NaN\n",
       "24         1  2012-04-11   NaN   NaN  NaN  1566.19  NaN      NaN\n",
       "25         2  2012-04-11   NaN   NaN  NaN    29.79  NaN      NaN\n",
       "26         3  2012-04-11   NaN   NaN  NaN  1534.99  NaN      NaN\n",
       "27         4  2012-04-11   NaN   NaN  NaN   616.47  NaN      NaN\n",
       "28         1  2012-04-12   NaN   NaN  NaN  1584.86  NaN      NaN\n",
       "29         2  2012-04-12   NaN   NaN  NaN    30.23  NaN      NaN\n",
       "...      ...         ...   ...   ...  ...      ...  ...      ...\n",
       "5902       3  2018-03-22   NaN   NaN  NaN  1748.70  NaN      NaN\n",
       "5903       4  2018-03-22   NaN   NaN  NaN  1822.64  NaN      NaN\n",
       "5904       1  2018-03-23   NaN   NaN  NaN  2428.36  NaN      NaN\n",
       "5905       2  2018-03-23   NaN   NaN  NaN    29.69  NaN      NaN\n",
       "5906       3  2018-03-23   NaN   NaN  NaN  1747.01  NaN      NaN\n",
       "5907       4  2018-03-23   NaN   NaN  NaN  1805.49  NaN      NaN\n",
       "5908       1  2018-03-24   NaN   NaN  NaN  2464.61  NaN      NaN\n",
       "5909       2  2018-03-24   NaN   NaN  NaN    30.33  NaN      NaN\n",
       "5910       3  2018-03-24   NaN   NaN  NaN  1751.58  NaN      NaN\n",
       "5911       4  2018-03-24   NaN   NaN  NaN  1802.99  NaN      NaN\n",
       "5912       1  2018-03-27   NaN   NaN  NaN  2471.24  NaN      NaN\n",
       "5913       2  2018-03-27   NaN   NaN  NaN    30.44  NaN      NaN\n",
       "5914       3  2018-03-27   NaN   NaN  NaN  1741.08  NaN      NaN\n",
       "5915       4  2018-03-27   NaN   NaN  NaN  1797.89  NaN      NaN\n",
       "5916       1  2018-03-28   NaN   NaN  NaN  2482.78  NaN      NaN\n",
       "5917       2  2018-03-28   NaN   NaN  NaN    30.59  NaN      NaN\n",
       "5918       3  2018-03-28   NaN   NaN  NaN  1759.16  NaN      NaN\n",
       "5919       4  2018-03-28   NaN   NaN  NaN  1797.77  NaN      NaN\n",
       "5920       1  2018-03-29   NaN   NaN  NaN  2481.73  NaN      NaN\n",
       "5921       2  2018-03-29   NaN   NaN  NaN    30.45  NaN      NaN\n",
       "5922       3  2018-03-29   NaN   NaN  NaN  1743.25  NaN      NaN\n",
       "5923       4  2018-03-29   NaN   NaN  NaN  1808.03  NaN      NaN\n",
       "5924       1  2018-03-30   NaN   NaN  NaN  2458.63  NaN      NaN\n",
       "5925       2  2018-03-30   NaN   NaN  NaN    30.23  NaN      NaN\n",
       "5926       3  2018-03-30   NaN   NaN  NaN  1738.25  NaN      NaN\n",
       "5927       4  2018-03-30   NaN   NaN  NaN  1803.25  NaN      NaN\n",
       "5928       1  2018-03-31   NaN   NaN  NaN  2437.35  NaN      NaN\n",
       "5929       2  2018-03-31   NaN   NaN  NaN    29.97  NaN      NaN\n",
       "5930       3  2018-03-31   NaN   NaN  NaN  1723.28  NaN      NaN\n",
       "5931       4  2018-03-31   NaN   NaN  NaN  1785.87  NaN      NaN\n",
       "\n",
       "[5932 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "train = time_preprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanEncodingTransforming(X, y, X_test, how_to_fill):\n",
    "    \n",
    "    # mean encoding for lgb\n",
    "    \n",
    "    X_train = pd.concat([X, y], axis=1)\n",
    "    mean_values = X_train.groupby(X_train.columns[0]).agg(how_to_fill).to_dict()['Value']\n",
    "    X_train = X_train.drop(y.columns[0], axis=1)\n",
    "    X_train = X_train.replace(mean_values)\n",
    "    X_test = X_test.replace(mean_values)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    X_train = pd.concat([X_train, train[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    X_test = pd.concat([X_test, test[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_groupby(train, test, window, how):\n",
    "    \n",
    "    # simple groupby prediction \n",
    "    \n",
    "    # time_delta = list((test['Timestamp'].iloc[-1:]  - train['Timestamp'].iloc[1] ).dt.days)[0]\n",
    "\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def train_mean(train, window):\n",
    "    \n",
    "    # return mean value from train for the window \n",
    "    \n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "   \n",
    "    return mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lgb(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    d2 = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1,\n",
    "        'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, valid_sets=d2, \n",
    "                    num_boost_round=50000, early_stopping_rounds=100)\n",
    "    \n",
    "    y_hat = gbm.predict(X_valid)\n",
    "    opt_boost_rounds = gbm.best_iteration\n",
    "    \n",
    "    return y_hat, opt_boost_rounds \n",
    "\n",
    "\n",
    "\n",
    "def train_lgb(X_train, y_train, X_test, opt_boost_rounds):\n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1,\n",
    "        'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, num_boost_round=opt_boost_rounds)\n",
    "    \n",
    "    y_hat = gbm.predict(X_test)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=142, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_y_hats(y_hats):\n",
    "    X_stack = pd.DataFrame({})\n",
    "    for i in range(0, len(y_hats)):\n",
    "        X_stack['stack'+str(i)] = y_hats[i]\n",
    "    return X_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stack(X_stack, y, model):\n",
    "    model.fit(X_stack, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_harmonic_features(x, col, period=24):\n",
    "    x['sin_'+col] = np.sin(x[col] * 2 * np.pi / period)\n",
    "    x['cos_'+col] = np.cos(x[col] * 2 * np.pi / period)\n",
    "    x = x.drop(col, axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minutes_in_day'] = train['hour']*60 + train['minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['holidays'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "def validate_stack(X_stack, y, model):\n",
    "    y_cros_val_pred = cross_val_predict(model, X_stack, y=y, cv=5, n_jobs=-1)\n",
    "    return y_cros_val_pred\n",
    "\n",
    "def cv_lr(X_stack, y, model):\n",
    "    \n",
    "    # cros val lasso on X_stack \n",
    "    \n",
    "    y_cros_val_pred = pd.DataFrame({})\n",
    "    coefs = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_stack):\n",
    "        X_train, X_test = X_stack.iloc[train_index], X_stack.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_temp = model \n",
    "        model_temp.fit(X_train,y_train)\n",
    "        y_hat = model_temp.predict(X_test)\n",
    "        coefs.append(model_temp.coef_)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'id': test_index, 'Value': y_hat})\n",
    "        y_cros_val_pred = pd.concat([y_cros_val_pred, temp_df], axis=0)\n",
    "      \n",
    "    y_cros_val_pred = y_cros_val_pred.sort_values(by='id')\n",
    "    \n",
    "    #divide by sum - due to overfit of model, with dividing - coefs ~ weights of models\n",
    "    return list(y_cros_val_pred['Value']) / (np.sum(np.mean(coefs,axis=0)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "print(train.shape)\n",
    "\n",
    "\n",
    "\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "#max_values = train[['Value', 'ForecastId']].groupby('ForecastId').agg('max').reset_index()\n",
    "#max_values.columns = ['ForecastId', 'max_value']\n",
    "#train = pd.merge(train, max_values, on = 'ForecastId', how='left')\n",
    "#train['Value'] = train['Value'] / train['max_value']\n",
    "#train = train.drop('max_value',axis=1)\n",
    "\n",
    "train['year_month_weekday'] = train['year'].astype('str') + '_' + train['month'].astype('str') + '_' + train['week_day'].astype('str') \n",
    "features = train[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "\n",
    "features = features.fillna(0)\n",
    "train = pd.merge(train, hol, on='Timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['week_day','month','year', 'week']\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[0:50]:\n",
    " \n",
    "    rf_stack = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "    lr_stack = Lasso(alpha=1, fit_intercept=False, max_iter=3000, tol=0.0001, positive=True, random_state=424142)\n",
    "\n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 99999\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    X_train['Value'] = abs(X_train['Value'])\n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "    \n",
    "    # drop outliers in train data set \n",
    "    up_border = X_train['Value'].quantile(0.985)\n",
    "    low_border = X_train['Value'].quantile(0.015)\n",
    "    X_train = X_train[(X_train['Value']<=up_border) & (X_train['Value']>=low_border)].reset_index(drop=True)\n",
    "    \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    X_test = pd.merge(X_test, hol, on='Timestamp', how='left')\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe')\n",
    "    X_train_meanenc, X_test_meanenc = feature_preprocessing(X_train, X_test, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    X_train_v_meanenc, X_valid_v_meanenc = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_mean = train_rf(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    y_rf_mean_log = np.exp(train_rf(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc)) - 1\n",
    "    \n",
    "        # LightGBM\n",
    "    #y_lgb, lgb_opt = validate_lgb(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe, X_valid_v['Value'])\n",
    "    #y_lgb_mean, lgb_mean_opt = validate_lgb(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc, X_valid_v['Value'])\n",
    "    #y_lgb_log, lgb_opt_log = validate_lgb(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe, X_valid_v['Value_log'])\n",
    "    #y_lgb_log = np.exp(y_lgb_log) -1\n",
    "    #y_lgb_mean_log, lgb_mean_opt_log = validate_lgb(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc, X_valid_v['Value_log'])\n",
    "    #y_lgb_mean_log = np.exp(y_lgb_mean_log) -1\n",
    "    \n",
    "    # stack predictions and make predictions \n",
    "    X_valid_stack = combine_y_hats([y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                                    y_hat_grby_median, y_hat_grby3_median, \n",
    "                                    y_rf, y_rf_mean, \n",
    "                                    y_rf_log, y_rf_mean_log, \n",
    "                                    #y_lgb, y_lgb_mean, \n",
    "                                   #y_lgb_log, y_lgb_mean_log\n",
    "                                   ])\n",
    "    #y_rf_hat = validate_stack(X_valid_stack, X_valid_v['Value'], rf_stack)\n",
    "    #y_lr_hat = cv_lr(X_valid_stack, X_valid_v['Value'], lr_stack)\n",
    " \n",
    "    # calculate scores and pick top model \n",
    "    iid = X_valid_v.reset_index()['index'] \n",
    "    T = np.max(iid)\n",
    "    index_mult = (3*T -2*iid +1) / 2 / T**2\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean = calc_score(y_rf_mean, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean_log = calc_score(y_rf_mean_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    #score_lgb = calc_score(y_lgb, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_mean = calc_score(y_lgb_mean, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_log = calc_score(y_lgb_log, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_mean_log = calc_score(y_lgb_mean_log, X_valid_v['Value'], index_mult)\n",
    "\n",
    "    #score_lr_stack = calc_score(y_lr_hat, X_valid_v['Value'], index_mult)\n",
    "    #score_rf_stack = calc_score(y_rf_hat, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    \n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, \n",
    "                  y_rf_mean, \n",
    "                  y_rf_log, \n",
    "                  y_rf_mean_log, \n",
    "                  #y_lgb, y_lgb_mean, \n",
    "                  #y_lgb_log, y_lgb_mean_log, \n",
    "                  #y_lr_hat, y_rf_hat\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, \n",
    "                  score_rf_mean, \n",
    "                  score_rf_log, \n",
    "                  score_rf_mean_log, \n",
    "                  #score_lgb, score_lgb_mean, \n",
    "                  #score_lgb_log, score_lgb_mean_log, \n",
    "                  #score_lr_stack\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_mean', \n",
    "                    'rf_log', 'rf_mean_log', \n",
    "                    #'lgb', 'lgb_mean', \n",
    "                    #'lgb_log', 'lgb_mean_log', \n",
    "                    #'lr_stack', 'rf_stack'\n",
    "                   ]\n",
    "    # plot figures and seve to folder \n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    ax.plot(y_hats[np.argmin(all_scores)])\n",
    "    ax.plot(X_valid_v['Value'].reset_index(drop=True))\n",
    "    fig.savefig('C:/Users/denis/Machine_Learning_Competitions/idao/lr_stack/'+str(i)+'.jpg')   # save the figure to file\n",
    "    # plt.show()\n",
    "    plt.close(fig)    # close the figure\n",
    "    \n",
    "    #calc R2 and save later \n",
    "    #r2 = r2_score(y_hats[np.argmin(all_scores)], X_valid_v['Value'] ) \n",
    "    #losses.append( r2 )\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "    y_hat = 0 \n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60 Best model is mean_all\n",
    "loss: 4305.94860943\n",
    "val score: 4158.48998272 + 2005.10425588\n",
    "{'rf_log': 4946.362227144722, 'median_3': 6169.4187582562745, 'rf_mean_log': 5304.017089875222, 'mean_3': 5520.169214317167, 'median_all': 4584.742404227213, 'mean_all': 4305.948609431632, 'rf_mean': 4971.130659230724, 'rf': 4742.6985121311545}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T15:32:14.797481Z",
     "start_time": "2018-04-03T15:28:49.796297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Best model is median_all\n",
      "loss: 2468.104184759202\n",
      "val score: 2468.104184759202 + 0.0\n",
      "{'mean_all': 2599.1933756945546, 'mean_3': 2962.694663908364, 'median_all': 2468.104184759202, 'median_3': 3016.6698438923136, 'rf': 2815.299635913476, 'rf_mean': 2940.2689446231193}\n",
      "-------------------------------\n",
      "2 Best model is mean_3\n",
      "loss: 7448.692853103659\n",
      "val score: 4958.398518931431 + 2490.294334172229\n",
      "{'mean_all': 10602.047360570356, 'mean_3': 7448.692853103659, 'median_all': 9239.853843142373, 'median_3': 7497.0638131280175, 'rf': 10661.805646178605, 'rf_mean': 7575.162500809586}\n",
      "-------------------------------\n",
      "3 Best model is mean_3\n",
      "loss: 3270.288558410898\n",
      "val score: 4395.69519875792 + 2183.494286751274\n",
      "{'mean_all': 3667.4469287290167, 'mean_3': 3270.288558410898, 'median_all': 3533.9007262830246, 'median_3': 3492.022916259417, 'rf': 3653.8107303429924, 'rf_mean': 5649.546611869762}\n",
      "-------------------------------\n",
      "4 Best model is median_all\n",
      "loss: 2762.278978388998\n",
      "val score: 3987.3411436656893 + 2018.9092545573703\n",
      "{'mean_all': 3375.9180132493407, 'mean_3': 3699.8760767719514, 'median_all': 2762.278978388998, 'median_3': 3177.875170016624, 'rf': 2938.407888771347, 'rf_mean': 4417.8301117005785}\n",
      "-------------------------------\n",
      "5 Best model is mean_3\n",
      "loss: 2976.0081112265702\n",
      "val score: 3785.0745371778657 + 1850.525001968446\n",
      "{'mean_all': 4840.472719865203, 'mean_3': 2976.0081112265702, 'median_all': 5308.953775642923, 'median_3': 3036.5486914453627, 'rf': 3970.7579500801708, 'rf_mean': 4331.733919459827}\n",
      "-------------------------------\n",
      "6 Best model is rf\n",
      "loss: 2105.369761101287\n",
      "val score: 3505.1237411651023 + 1801.5450480165155\n",
      "{'mean_all': 2415.3059478234136, 'mean_3': 2244.7719624544125, 'median_all': 2272.232515869046, 'median_3': 2354.3957785871794, 'rf': 2105.369761101287, 'rf_mean': 2869.104758946771}\n",
      "-------------------------------\n",
      "7 Best model is rf_mean\n",
      "loss: 4581.430570586662\n",
      "val score: 3658.8818596538963 + 1709.9011182040392\n",
      "{'mean_all': 6284.006212170051, 'mean_3': 6577.753994984629, 'median_all': 6196.134596619346, 'median_3': 6453.146148761371, 'rf': 4615.75356479587, 'rf_mean': 4581.430570586662}\n",
      "-------------------------------\n",
      "8 Best model is median_3\n",
      "loss: 6807.352639145615\n",
      "val score: 4052.4407070903617 + 1908.5364994019963\n",
      "{'mean_all': 8147.957661620534, 'mean_3': 9142.808242623398, 'median_all': 7559.047032244815, 'median_3': 6807.352639145615, 'rf': 10019.515472033958, 'rf_mean': 7409.906572746737}\n",
      "-------------------------------\n",
      "9 Best model is median_all\n",
      "loss: 4821.205865909775\n",
      "val score: 4137.859058070297 + 1815.5325442676428\n",
      "{'mean_all': 4883.7950404657195, 'mean_3': 4949.010163563125, 'median_all': 4821.205865909775, 'median_3': 4944.308261299617, 'rf': 6082.20623862154, 'rf_mean': 6917.2737352701415}\n",
      "-------------------------------\n",
      "10 Best model is rf\n",
      "loss: 4092.1600682417024\n",
      "val score: 4133.289159087437 + 1722.419964223554\n",
      "{'mean_all': 6525.488136213954, 'mean_3': 7748.830565177489, 'median_all': 7275.734967581491, 'median_3': 10000.0, 'rf': 4092.1600682417024, 'rf_mean': 7613.19554284349}\n",
      "-------------------------------\n",
      "11 Best model is mean_all\n",
      "loss: 3199.235594462623\n",
      "val score: 4048.375198667 + 1664.070780993784\n",
      "{'mean_all': 3199.235594462623, 'mean_3': 3291.3774630828707, 'median_all': 3460.427633111287, 'median_3': 4175.711557273956, 'rf': 3252.6358286362015, 'rf_mean': 5851.452498048201}\n",
      "-------------------------------\n",
      "14 Best model is rf_mean\n",
      "loss: 4831.130334116397\n",
      "val score: 4113.604793287783 + 1607.8477852818653\n",
      "{'mean_all': 6740.697914034038, 'mean_3': 6077.471816060104, 'median_all': 5860.733813305623, 'median_3': 5269.056631782083, 'rf': 5482.87727718812, 'rf_mean': 4831.130334116397}\n",
      "-------------------------------\n",
      "17 Best model is median_3\n",
      "loss: 2607.8113729617235\n",
      "val score: 3997.7745301857785 + 1596.030927603588\n",
      "{'mean_all': 7047.366954730161, 'mean_3': 3253.6362249141166, 'median_all': 9176.032082002355, 'median_3': 2607.8113729617235, 'rf': 4645.375303570092, 'rf_mean': 8857.896665357639}\n",
      "-------------------------------\n",
      "18 Best model is median_3\n",
      "loss: 4704.443784689549\n",
      "val score: 4048.250905507476 + 1548.7045904104014\n",
      "{'mean_all': 8373.154463603876, 'mean_3': 4777.334400123453, 'median_all': 9236.641221374046, 'median_3': 4704.443784689549, 'rf': 6026.396136408239, 'rf_mean': 8993.49618648568}\n",
      "-------------------------------\n",
      "19 Best model is mean_3\n",
      "loss: 2558.131474594067\n",
      "val score: 3948.9096101132486 + 1541.6707006976153\n",
      "{'mean_all': 2598.7835396912333, 'mean_3': 2558.131474594067, 'median_all': 2588.116426835666, 'median_3': 2609.770613821511, 'rf': 2846.1475503676465, 'rf_mean': 2873.8574983526682}\n",
      "-------------------------------\n",
      "21 Best model is median_3\n",
      "loss: 5634.272075794013\n",
      "val score: 4054.244764218296 + 1547.4605602107106\n",
      "{'mean_all': 5891.79773995435, 'mean_3': 6155.460920730883, 'median_all': 6267.938381017574, 'median_3': 5634.272075794013, 'rf': 6029.312480691063, 'rf_mean': 7851.086283844843}\n",
      "-------------------------------\n",
      "22 Best model is median_all\n",
      "loss: 8118.231160853285\n",
      "val score: 4293.302787549766 + 1779.9306636153824\n",
      "{'mean_all': 8217.72249097648, 'mean_3': 8530.618083897842, 'median_all': 8118.231160853285, 'median_3': 9021.247948836623, 'rf': 10317.09828754155, 'rf_mean': 9862.54435505254}\n",
      "-------------------------------\n",
      "23 Best model is median_all\n",
      "loss: 8641.228115746273\n",
      "val score: 4534.854194671794 + 1996.007278741688\n",
      "{'mean_all': 9552.115727202005, 'mean_3': 10063.934057180459, 'median_all': 8641.228115746273, 'median_3': 8716.816457997818, 'rf': 11194.525256723291, 'rf_mean': 8891.149957087366}\n",
      "-------------------------------\n",
      "24 Best model is median_3\n",
      "loss: 1724.7521126604263\n",
      "val score: 4386.9540850922485 + 2041.5921331889583\n",
      "{'mean_all': 5306.870524751438, 'mean_3': 1787.4115216518796, 'median_all': 6776.077415202278, 'median_3': 1724.7521126604263, 'rf': 3363.2726307593, 'rf_mean': 9672.557275649986}\n",
      "-------------------------------\n",
      "25 Best model is median_3\n",
      "loss: 2136.409227683049\n",
      "val score: 4274.426842221788 + 2049.4581585767037\n",
      "{'mean_all': 2282.1509981328045, 'mean_3': 2142.9783857065704, 'median_all': 2252.5819215889424, 'median_3': 2136.409227683049, 'rf': 2458.1666244153707, 'rf_mean': 2345.426803126357}\n",
      "-------------------------------\n",
      "26 Best model is rf\n",
      "loss: 2399.914043669361\n",
      "val score: 4185.164328005007 + 2039.5149911511849\n",
      "{'mean_all': 2866.486120608222, 'mean_3': 3376.656111662492, 'median_all': 2818.7768798147363, 'median_3': 3185.169431447611, 'rf': 2399.914043669361, 'rf_mean': 2940.341269221606}\n",
      "-------------------------------\n",
      "27 Best model is median_3\n",
      "loss: 1428.5260078850313\n",
      "val score: 4059.8625861813707 + 2073.70652594689\n",
      "{'mean_all': 1488.57892708117, 'mean_3': 1498.7759125015896, 'median_all': 1438.7399847386496, 'median_3': 1428.5260078850313, 'rf': 1555.4926289796094, 'rf_mean': 1576.841847487775}\n",
      "-------------------------------\n",
      "28 Best model is mean_all\n",
      "loss: 2510.5524228114823\n",
      "val score: 3992.501274730506 + 2052.587879223141\n",
      "{'mean_all': 2510.5524228114823, 'mean_3': 2599.247786576962, 'median_all': 2629.7684958067607, 'median_3': 2536.501039549606, 'rf': 2735.7251734529395, 'rf_mean': 2733.865541432319}\n",
      "-------------------------------\n",
      "29 Best model is rf\n",
      "loss: 3254.4530363850013\n",
      "val score: 3961.749264799444 + 2014.7757296599832\n",
      "{'mean_all': 4378.916088461351, 'mean_3': 3482.500288662035, 'median_all': 4560.384701591377, 'median_3': 3473.1136785052063, 'rf': 3254.4530363850013, 'rf_mean': 4187.33100857553}\n",
      "-------------------------------\n",
      "30 Best model is median_3\n",
      "loss: 2289.7641126899616\n",
      "val score: 3894.869858715065 + 2001.0739392112987\n",
      "{'mean_all': 2541.014418236727, 'mean_3': 2438.4189556080537, 'median_all': 2621.087643830142, 'median_3': 2289.7641126899616, 'rf': 2439.9708715803, 'rf_mean': 2922.1655182106965}\n",
      "-------------------------------\n",
      "31 Best model is rf_mean\n",
      "loss: 5806.031094130669\n",
      "val score: 3968.3760600772034 + 1996.3377799198183\n",
      "{'mean_all': 5887.3332117955915, 'mean_3': 6088.69472099574, 'median_all': 6121.604109599671, 'median_3': 6246.420783896826, 'rf': 5892.039267266241, 'rf_mean': 5806.031094130669}\n",
      "-------------------------------\n",
      "32 Best model is median_3\n",
      "loss: 2133.514825116885\n",
      "val score: 3900.418236560155 + 1989.430407926138\n",
      "{'mean_all': 4594.427574215481, 'mean_3': 2342.085708837203, 'median_all': 4811.209090371072, 'median_3': 2133.514825116885, 'rf': 2713.348194617059, 'rf_mean': 5731.408958300837}\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Best model is rf_mean\n",
      "loss: 11204.690754931626\n",
      "val score: 4161.285112216278 + 2377.787497018121\n",
      "{'mean_all': 17107.731997159473, 'mean_3': 18146.82882176017, 'median_all': 15162.024323264026, 'median_3': 14438.603373872107, 'rf': 14715.924545573429, 'rf_mean': 11204.690754931626}\n",
      "-------------------------------\n",
      "34 Best model is median_all\n",
      "loss: 5385.501901578887\n",
      "val score: 4203.499484263265 + 2347.0853669418525\n",
      "{'mean_all': 5486.808936180766, 'mean_3': 5451.576965925242, 'median_all': 5385.501901578887, 'median_3': 5655.180361876224, 'rf': 5480.00460988821, 'rf_mean': 5695.740814620448}\n",
      "-------------------------------\n",
      "35 Best model is mean_all\n",
      "loss: 2355.7750378307223\n",
      "val score: 4141.90866938218 + 2331.349856823135\n",
      "{'mean_all': 2355.7750378307223, 'mean_3': 2611.873757354089, 'median_all': 2844.972076516452, 'median_3': 2588.7071053899444, 'rf': 2779.2566065220394, 'rf_mean': 2651.841025295397}\n",
      "-------------------------------\n",
      "36 Best model is median_all\n",
      "loss: 3044.0093689412874\n",
      "val score: 4106.492562916345 + 2301.6282049094316\n",
      "{'mean_all': 3246.021473351275, 'mean_3': 3638.167602440109, 'median_all': 3044.0093689412874, 'median_3': 4184.53228138203, 'rf': 4012.8472728087777, 'rf_mean': 3794.7528536930713}\n",
      "-------------------------------\n",
      "38 Best model is rf_mean\n",
      "loss: 1737.3456835662685\n",
      "val score: 4032.456722936655 + 2302.5781702213662\n",
      "{'mean_all': 1737.531056414165, 'mean_3': 2129.8833562559785, 'median_all': 1836.3418313686486, 'median_3': 2281.524982510577, 'rf': 2015.7567407647123, 'rf_mean': 1737.3456835662685}\n",
      "-------------------------------\n",
      "40 Best model is mean_all\n",
      "loss: 3701.368549686721\n",
      "val score: 4022.423747989687 + 2268.1324378685313\n",
      "{'mean_all': 3701.368549686721, 'mean_3': 4255.50272935376, 'median_all': 3747.3586899101956, 'median_3': 4874.537770734284, 'rf': 4385.42216939602, 'rf_mean': 5644.4126077138035}\n",
      "-------------------------------\n",
      "41 Best model is mean_3\n",
      "loss: 1884.933039434346\n",
      "val score: 3959.5563742086474 + 2263.5247438213414\n",
      "{'mean_all': 2959.411510533701, 'mean_3': 1884.933039434346, 'median_all': 3023.051071470495, 'median_3': 1988.2527357387255, 'rf': 2512.510995850923, 'rf_mean': 2873.644086415101}\n",
      "-------------------------------\n",
      "43 Best model is median_all\n",
      "loss: 6618.274653254077\n",
      "val score: 4035.5197536099454 + 2274.5004016724693\n",
      "{'mean_all': 6876.122740050047, 'mean_3': 7725.346745922878, 'median_all': 6618.274653254077, 'median_3': 7864.654778235025, 'rf': 6987.9686785550975, 'rf_mean': 7196.652073402019}\n",
      "-------------------------------\n",
      "44 Best model is median_3\n",
      "loss: 5981.354755529108\n",
      "val score: 4089.5707258854777 + 2265.369863615849\n",
      "{'mean_all': 6063.829393251205, 'mean_3': 6028.100102482727, 'median_all': 6360.210254884459, 'median_3': 5981.354755529108, 'rf': 6149.657839928594, 'rf_mean': 6093.785451072608}\n",
      "-------------------------------\n",
      "45 Best model is median_all\n",
      "loss: 4000.595622176999\n",
      "val score: 4087.1659933528163 + 2234.5936500486314\n",
      "{'mean_all': 4105.970326957621, 'mean_3': 5515.163547922768, 'median_all': 4000.595622176999, 'median_3': 4629.969722539336, 'rf': 4120.809963435417, 'rf_mean': 4220.51491498692}\n",
      "-------------------------------\n",
      "47 Best model is median_all\n",
      "loss: 3709.5506353989254\n",
      "val score: 4077.228747090872 + 2205.823427780679\n",
      "{'mean_all': 4351.943882906832, 'mean_3': 3921.4812873924625, 'median_all': 3709.5506353989254, 'median_3': 4420.280361587842, 'rf': 4799.647364513733, 'rf_mean': 4108.1138063909975}\n",
      "-------------------------------\n",
      "48 Best model is median_3\n",
      "loss: 5552.482876028038\n",
      "val score: 4115.055776037979 + 2189.8105295456935\n",
      "{'mean_all': 5778.076378737349, 'mean_3': 6520.478165770172, 'median_all': 9446.917685046796, 'median_3': 5552.482876028038, 'rf': 5997.726461365494, 'rf_mean': 8916.169473541731}\n",
      "-------------------------------\n",
      "49 Best model is median_all\n",
      "loss: 5540.839963742572\n",
      "val score: 4150.700380730594 + 2173.692567344904\n",
      "{'mean_all': 5600.737387348912, 'mean_3': 5810.319938228086, 'median_all': 5540.839963742572, 'median_3': 5646.087219256722, 'rf': 6016.857186020748, 'rf_mean': 5813.289580699838}\n",
      "-------------------------------\n",
      "50 Best model is rf\n",
      "loss: 1761.721812451001\n",
      "val score: 4092.432610772555 + 2178.417378311252\n",
      "{'mean_all': 2113.189201631002, 'mean_3': 2325.221463123257, 'median_all': 2061.6470136477815, 'median_3': 2257.7238764179774, 'rf': 1761.721812451001, 'rf_mean': 2354.533769007138}\n",
      "-------------------------------\n",
      "51 Best model is median_all\n",
      "loss: 1804.9034602053016\n",
      "val score: 4037.967630997144 + 2180.3985689877504\n",
      "{'mean_all': 1996.321145203098, 'mean_3': 2374.658554340784, 'median_all': 1804.9034602053016, 'median_3': 2025.2319159025185, 'rf': 2546.8824091203555, 'rf_mean': 5031.917664595659}\n",
      "-------------------------------\n",
      "52 Best model is median_all\n",
      "loss: 2979.999491727871\n",
      "val score: 4013.36372078158 + 2160.7872115011655\n",
      "{'mean_all': 4161.749884432429, 'mean_3': 3208.9252585834456, 'median_all': 2979.999491727871, 'median_3': 3217.616711987598, 'rf': 5159.475674943033, 'rf_mean': 7162.102937576153}\n",
      "-------------------------------\n",
      "53 Best model is median_3\n",
      "loss: 5270.915720815572\n",
      "val score: 4041.9444480550796 + 2144.2976935546376\n",
      "{'mean_all': 6186.621096352967, 'mean_3': 5418.108982589492, 'median_all': 5547.994453289508, 'median_3': 5270.915720815572, 'rf': 6257.641706469449, 'rf_mean': 6210.9096277961935}\n",
      "-------------------------------\n",
      "54 Best model is rf\n",
      "loss: 2343.284725871431\n",
      "val score: 4004.1964542287765 + 2135.0715740942974\n",
      "{'mean_all': 2417.881645347896, 'mean_3': 2411.761156847862, 'median_all': 2344.9524143165763, 'median_3': 2364.424592748292, 'rf': 2343.284725871431, 'rf_mean': 2553.853933678306}\n",
      "-------------------------------\n",
      "55 Best model is mean_all\n",
      "loss: 3300.63237717404\n",
      "val score: 3988.901582988456 + 2114.227784205514\n",
      "{'mean_all': 3300.63237717404, 'mean_3': 4191.224939307705, 'median_all': 3931.6548497564268, 'median_3': 3741.860801620605, 'rf': 3966.5800495184812, 'rf_mean': 3939.2743266557604}\n",
      "-------------------------------\n",
      "56 Best model is median_all\n",
      "loss: 2658.160191249927\n",
      "val score: 3960.587936355721 + 2100.4118856024534\n",
      "{'mean_all': 3140.455168233066, 'mean_3': 2714.847136110085, 'median_all': 2658.160191249927, 'median_3': 2751.783250082603, 'rf': 4055.8089249965988, 'rf_mean': 9698.369720229428}\n",
      "-------------------------------\n",
      "57 Best model is rf_mean\n",
      "loss: 5395.397335991714\n",
      "val score: 3990.479798848138 + 2088.495785844192\n",
      "{'mean_all': 6623.013402080593, 'mean_3': 5841.19117550598, 'median_all': 5656.698904590807, 'median_3': 6266.86296088449, 'rf': 5916.938453988685, 'rf_mean': 5395.397335991714}\n",
      "-------------------------------\n",
      "58 Best model is rf\n",
      "loss: 3462.3328943923366\n",
      "val score: 3979.701290593938 + 2068.423187251321\n",
      "{'mean_all': 4078.003485209063, 'mean_3': 3799.149840595112, 'median_all': 3856.327681005406, 'median_3': 3895.254816799889, 'rf': 3462.3328943923366, 'rf_mean': 5982.059919904537}\n",
      "-------------------------------\n",
      "60 Best model is median_all\n",
      "loss: 14014.959685480108\n",
      "val score: 4180.406458491661 + 2483.27458178703\n",
      "{'mean_all': 14483.110360734612, 'mean_3': 20871.17123120322, 'median_all': 14014.959685480108, 'median_3': 22378.88985140268, 'rf': 20769.689422713844, 'rf_mean': 16413.232829462817}\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "hol = pd.read_csv('hol.csv')\n",
    "hol['Timestamp'] = pd.to_datetime(hol['Timestamp'])\n",
    "\n",
    "\n",
    "\n",
    "def replace_week_ago(trainnine):\n",
    "    fist_q =  trainnine.CLIENT_OUT.quantile(0.9)\n",
    "    last_q = trainnine.CLIENT_OUT.quantile(0.1)\n",
    "    mask = (trainnine.CLIENT_OUT > fist_q) | (trainnine.CLIENT_OUT < last_q) \n",
    "    trainnine['mask'] = mask\n",
    "    trainnine['shifted'] = trainnine.CLIENT_OUT.shift(7)\n",
    "    trainnine.loc[mask,['CLIENT_OUT','shifted', ]] = trainnine.loc[mask,['shifted', 'CLIENT_OUT']].values\n",
    "    trainnine = trainnine.iloc[7:]\n",
    "    return trainnine[['DATE', \"ATM_ID\", 'CLIENT_OUT']]\n",
    "\n",
    "\n",
    "def apply_replace(train):\n",
    "    train2 = train.iloc[:0].copy()\n",
    "    ATM_IDs = train.ATM_ID.unique()\n",
    "    for ATM in ATM_IDs:\n",
    "        trainnine = train[(train.ATM_ID==ATM)].copy()\n",
    "        trainnine = replace_week_ago(trainnine)\n",
    "    #     print(trainnine.head())\n",
    "        train2 = pd.concat([train2, trainnine])\n",
    "    train = train2.copy()\n",
    "    del train2\n",
    "    return train\n",
    "train = apply_replace(train)\n",
    "\n",
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    X_train = pd.concat([X_train, train[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    X_test = pd.concat([X_test, test[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test \n",
    "\n",
    "def train_groupby(train, test, window, how):\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def train_mean(train, window):\n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "    return mean_value\n",
    "\n",
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=240, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "# val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "# val['Value'] = val['Value'].fillna(0)\n",
    "# val['Value'] = (val['Value']==0) * 1\n",
    "# val.columns = ['diff']\n",
    "# train = pd.concat([train, val], axis=1)\n",
    "\n",
    "\n",
    "# a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "# a.columns = ['ForecastId', 'to_drop']\n",
    "# a = a['to_drop']\n",
    "# train = pd.concat([train, a], axis=1)\n",
    "\n",
    "# print(train.shape)\n",
    "# train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "# train = train[['Timestamp','ForecastId','Value']]\n",
    "train = time_preprocess(train)\n",
    "\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "train['year_month_weekday'] = train['year'].astype('str') + '_' + train['month'].astype('str') + '_' + train['week_day'].astype('str') \n",
    "features = train[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "features = features.fillna(0)\n",
    "train = pd.merge(train, hol, on='Timestamp', how='left')\n",
    "\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['week_day','month','year','week']\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "index_mult = 0\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[0:50]:\n",
    "    \n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 99999\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    X_train['Value'] = abs(X_train['Value'])\n",
    "       \n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "     \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    X_test = pd.merge(X_test, hol, on='Timestamp', how='left')\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe')\n",
    "  \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], 0)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], 0)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], 0)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], 0)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    \n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, y_rf_log, \n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, \n",
    "                  score_rf_log, \n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_mean', \n",
    "                   ]\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==bst_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "\n",
    "    if (score_rf==beste_score):\n",
    "        y_hat = train_rf(X_train_ohe, X_train['Value'], X_test_ohe)\n",
    "        \n",
    "    if (score_rf_log==best_score):\n",
    "        y_hat = np.exp(train_rf(X_train_ohe, X_train['Value_log'], X_test_ohe)) -1    \n",
    "        \n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    ax.plot(y_hats[np.argmin(all_scores)])\n",
    "    ax.plot(X_valid_v['Value'].reset_index(drop=True))\n",
    "    fig.savefig('pics/'+str(i)+'.jpg')   # save the figure to file\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "sub = sub[['Timestamp', 'ForecastId', 'Value']]\n",
    "sub.columns = ['DATE', 'ATM_ID', 'CLIENT_OUT']\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
