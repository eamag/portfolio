{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.optimize import minimize, fmin_slsqp\n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pylab\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import lightgbm as lgb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train and sub format already merged with weather and holidays (plus the day before holiday)\n",
    "train = pd.read_csv('train.csv')\n",
    "hol = pd.read_csv('hol.csv')\n",
    "hol['Timestamp'] = pd.to_datetime(hol['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gold and usd preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv('gold_usd/gold.csv')\n",
    "usd = pd.read_csv('gold_usd/usd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold['DATE'] = pd.to_datetime(gold['DATE'])\n",
    "usd['DATE'] = pd.to_datetime(usd['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gold.shape)\n",
    "gold = gold[(gold.TICKER==1) & (gold.DATE>=pd.to_datetime('2015-01-01')) & (gold.DATE<=pd.to_datetime('2017-09-17'))]\n",
    "gold = gold[['DATE','CLOSE']].reset_index(drop=True)\n",
    "print(gold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usd.shape)\n",
    "usd = usd[(usd.DATE>=pd.to_datetime('2015-01-01')) & (usd.DATE<=pd.to_datetime('2017-09-17'))]\n",
    "usd = usd[(usd.TICKER=='USD') | (usd.TICKER=='EUR')]\n",
    "usd = usd[['DATE','CLOSE','TICKER']].reset_index(drop=True)\n",
    "print(gold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd = usd.pivot(index='DATE', columns='TICKER', values='CLOSE')\n",
    "usd = usd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pd.to_datetime('2015-01-01')\n",
    "max_date = pd.to_datetime('2017-09-17')\n",
    "dates = pd.date_range(start=min_date, end=max_date, periods=None, freq='D')\n",
    "h = pd.DataFrame({'DATE': dates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = pd.merge(h, usd, how='left', on='DATE')\n",
    "h = pd.merge(h, gold, how='left', on='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.columns = ['Timestamp','EUR','USD','CLOSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['EUR', 'USD', 'CLOSE']:\n",
    "    h[i] = h[i].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for i in [1,7,14,30]:\n",
    "    for j in ['EUR', 'USD', 'CLOSE']:\n",
    "        new_col = j + '_' + str(i)\n",
    "        new_cols.append(new_col)\n",
    "        h[new_col] = h[j].diff(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h.fillna(0)\n",
    "h.to_csv('golds.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanEncodingTransforming(X, y, X_test, how_to_fill):\n",
    "    \n",
    "    # mean encoding for lgb\n",
    "    \n",
    "    X_train = pd.concat([X, y], axis=1)\n",
    "    mean_values = X_train.groupby(X_train.columns[0]).agg(how_to_fill).to_dict()['Value']\n",
    "    X_train = X_train.drop(y.columns[0], axis=1)\n",
    "    X_train = X_train.replace(mean_values)\n",
    "    X_test = X_test.replace(mean_values)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    X_train = pd.concat([X_train, train[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    X_test = pd.concat([X_test, test[['days_to_holiday', 'days_after_holiday']]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_groupby(train, test, window, how):\n",
    "    \n",
    "    # simple groupby prediction \n",
    "    \n",
    "    # time_delta = list((test['Timestamp'].iloc[-1:]  - train['Timestamp'].iloc[1] ).dt.days)[0]\n",
    "\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def train_mean(train, window):\n",
    "    \n",
    "    # return mean value from train for the window \n",
    "    \n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "   \n",
    "    return mean_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lgb(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    d2 = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1,\n",
    "        'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, valid_sets=d2, \n",
    "                    num_boost_round=50000, early_stopping_rounds=100)\n",
    "    \n",
    "    y_hat = gbm.predict(X_valid)\n",
    "    opt_boost_rounds = gbm.best_iteration\n",
    "    \n",
    "    return y_hat, opt_boost_rounds \n",
    "\n",
    "\n",
    "\n",
    "def train_lgb(X_train, y_train, X_test, opt_boost_rounds):\n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        'min_data':1,\n",
    "        'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, num_boost_round=opt_boost_rounds)\n",
    "    \n",
    "    y_hat = gbm.predict(X_test)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=142, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_y_hats(y_hats):\n",
    "    X_stack = pd.DataFrame({})\n",
    "    for i in range(0, len(y_hats)):\n",
    "        X_stack['stack'+str(i)] = y_hats[i]\n",
    "    return X_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stack(X_stack, y, model):\n",
    "    model.fit(X_stack, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_harmonic_features(x, col, period=24):\n",
    "    x['sin_'+col] = np.sin(x[col] * 2 * np.pi / period)\n",
    "    x['cos_'+col] = np.cos(x[col] * 2 * np.pi / period)\n",
    "    x = x.drop(col, axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minutes_in_day'] = train['hour']*60 + train['minute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['holidays'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "def validate_stack(X_stack, y, model):\n",
    "    y_cros_val_pred = cross_val_predict(model, X_stack, y=y, cv=5, n_jobs=-1)\n",
    "    return y_cros_val_pred\n",
    "\n",
    "def cv_lr(X_stack, y, model):\n",
    "    \n",
    "    # cros val lasso on X_stack \n",
    "    \n",
    "    y_cros_val_pred = pd.DataFrame({})\n",
    "    coefs = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_stack):\n",
    "        X_train, X_test = X_stack.iloc[train_index], X_stack.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_temp = model \n",
    "        model_temp.fit(X_train,y_train)\n",
    "        y_hat = model_temp.predict(X_test)\n",
    "        coefs.append(model_temp.coef_)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'id': test_index, 'Value': y_hat})\n",
    "        y_cros_val_pred = pd.concat([y_cros_val_pred, temp_df], axis=0)\n",
    "      \n",
    "    y_cros_val_pred = y_cros_val_pred.sort_values(by='id')\n",
    "    \n",
    "    #divide by sum - due to overfit of model, with dividing - coefs ~ weights of models\n",
    "    return list(y_cros_val_pred['Value']) / (np.sum(np.mean(coefs,axis=0)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "print(train.shape)\n",
    "\n",
    "\n",
    "\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "#max_values = train[['Value', 'ForecastId']].groupby('ForecastId').agg('max').reset_index()\n",
    "#max_values.columns = ['ForecastId', 'max_value']\n",
    "#train = pd.merge(train, max_values, on = 'ForecastId', how='left')\n",
    "#train['Value'] = train['Value'] / train['max_value']\n",
    "#train = train.drop('max_value',axis=1)\n",
    "\n",
    "train['year_month_weekday'] = train['year'].astype('str') + '_' + train['month'].astype('str') + '_' + train['week_day'].astype('str') \n",
    "features = train[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "\n",
    "features = features.fillna(0)\n",
    "train = pd.merge(train, hol, on='Timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['week_day','month','year', 'week']\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[0:50]:\n",
    " \n",
    "    rf_stack = RandomForestRegressor(n_estimators=1000, random_state=42, n_jobs=-1)\n",
    "    lr_stack = Lasso(alpha=1, fit_intercept=False, max_iter=3000, tol=0.0001, positive=True, random_state=424142)\n",
    "\n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 99999\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    X_train['Value'] = abs(X_train['Value'])\n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "    \n",
    "    # drop outliers in train data set \n",
    "    up_border = X_train['Value'].quantile(0.985)\n",
    "    low_border = X_train['Value'].quantile(0.015)\n",
    "    X_train = X_train[(X_train['Value']<=up_border) & (X_train['Value']>=low_border)].reset_index(drop=True)\n",
    "    \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    X_test = pd.merge(X_test, hol, on='Timestamp', how='left')\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe')\n",
    "    X_train_meanenc, X_test_meanenc = feature_preprocessing(X_train, X_test, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    X_train_v_meanenc, X_valid_v_meanenc = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='mean_enc')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_mean = train_rf(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    y_rf_mean_log = np.exp(train_rf(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc)) - 1\n",
    "    \n",
    "        # LightGBM\n",
    "    #y_lgb, lgb_opt = validate_lgb(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe, X_valid_v['Value'])\n",
    "    #y_lgb_mean, lgb_mean_opt = validate_lgb(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc, X_valid_v['Value'])\n",
    "    #y_lgb_log, lgb_opt_log = validate_lgb(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe, X_valid_v['Value_log'])\n",
    "    #y_lgb_log = np.exp(y_lgb_log) -1\n",
    "    #y_lgb_mean_log, lgb_mean_opt_log = validate_lgb(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc, X_valid_v['Value_log'])\n",
    "    #y_lgb_mean_log = np.exp(y_lgb_mean_log) -1\n",
    "    \n",
    "    # stack predictions and make predictions \n",
    "    X_valid_stack = combine_y_hats([y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                                    y_hat_grby_median, y_hat_grby3_median, \n",
    "                                    y_rf, y_rf_mean, \n",
    "                                    y_rf_log, y_rf_mean_log, \n",
    "                                    #y_lgb, y_lgb_mean, \n",
    "                                   #y_lgb_log, y_lgb_mean_log\n",
    "                                   ])\n",
    "    #y_rf_hat = validate_stack(X_valid_stack, X_valid_v['Value'], rf_stack)\n",
    "    #y_lr_hat = cv_lr(X_valid_stack, X_valid_v['Value'], lr_stack)\n",
    " \n",
    "    # calculate scores and pick top model \n",
    "    iid = X_valid_v.reset_index()['index'] \n",
    "    T = np.max(iid)\n",
    "    index_mult = (3*T -2*iid +1) / 2 / T**2\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], index_mult)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], index_mult)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean = calc_score(y_rf_mean, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    score_rf_mean_log = calc_score(y_rf_mean_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    #score_lgb = calc_score(y_lgb, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_mean = calc_score(y_lgb_mean, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_log = calc_score(y_lgb_log, X_valid_v['Value'], index_mult)\n",
    "    #score_lgb_mean_log = calc_score(y_lgb_mean_log, X_valid_v['Value'], index_mult)\n",
    "\n",
    "    #score_lr_stack = calc_score(y_lr_hat, X_valid_v['Value'], index_mult)\n",
    "    #score_rf_stack = calc_score(y_rf_hat, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    \n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, \n",
    "                  y_rf_mean, \n",
    "                  y_rf_log, \n",
    "                  y_rf_mean_log, \n",
    "                  #y_lgb, y_lgb_mean, \n",
    "                  #y_lgb_log, y_lgb_mean_log, \n",
    "                  #y_lr_hat, y_rf_hat\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, \n",
    "                  score_rf_mean, \n",
    "                  score_rf_log, \n",
    "                  score_rf_mean_log, \n",
    "                  #score_lgb, score_lgb_mean, \n",
    "                  #score_lgb_log, score_lgb_mean_log, \n",
    "                  #score_lr_stack\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_mean', \n",
    "                    'rf_log', 'rf_mean_log', \n",
    "                    #'lgb', 'lgb_mean', \n",
    "                    #'lgb_log', 'lgb_mean_log', \n",
    "                    #'lr_stack', 'rf_stack'\n",
    "                   ]\n",
    "    # plot figures and seve to folder \n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    ax.plot(y_hats[np.argmin(all_scores)])\n",
    "    ax.plot(X_valid_v['Value'].reset_index(drop=True))\n",
    "    fig.savefig('C:/Users/denis/Machine_Learning_Competitions/idao/lr_stack/'+str(i)+'.jpg')   # save the figure to file\n",
    "    # plt.show()\n",
    "    plt.close(fig)    # close the figure\n",
    "    \n",
    "    #calc R2 and save later \n",
    "    #r2 = r2_score(y_hats[np.argmin(all_scores)], X_valid_v['Value'] ) \n",
    "    #losses.append( r2 )\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "    y_hat = 0 \n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60 Best model is mean_all\n",
    "loss: 4305.94860943\n",
    "val score: 4158.48998272 + 2005.10425588\n",
    "{'rf_log': 4946.362227144722, 'median_3': 6169.4187582562745, 'rf_mean_log': 5304.017089875222, 'mean_3': 5520.169214317167, 'median_all': 4584.742404227213, 'mean_all': 4305.948609431632, 'rf_mean': 4971.130659230724, 'rf': 4742.6985121311545}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "hol = pd.read_csv('hol.csv')\n",
    "h = pd.read_csv('golds.csv')\n",
    "hol['Timestamp'] = pd.to_datetime(hol['Timestamp'])\n",
    "h['Timestamp'] = pd.to_datetime(h['Timestamp'])\n",
    "\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    if (cat_type=='harmonic'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        temp_train = ohe.fit_transform(train[['year']])\n",
    "        temp_test = ohe.transform(test[['year']])\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(temp_train)],axis=1)\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(temp_test)],axis=1)\n",
    "        for i in range(len(cat_cols)):\n",
    "            X_train = make_harmonic_features(X_train, cat_cols[i], period=harmonic_features_max[i])\n",
    "            X_test = make_harmonic_features(X_test, cat_cols[i], period=harmonic_features_max[i])\n",
    "        \n",
    "    #X_train = pd.concat([X_train, train[new_cols]], axis=1).fillna(-999)\n",
    "    #X_test = pd.concat([X_test, test[new_cols]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test \n",
    "\n",
    "def train_groupby(train, test, window, how):\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def train_mean(train, window):\n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "    return mean_value\n",
    "\n",
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=180, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    \n",
    "    return X\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "train = time_preprocess(train)\n",
    "print(train.shape)\n",
    "\n",
    "train = pd.merge(train, hol, on='Timestamp', how='left')\n",
    "train = pd.merge(train, h, on='Timestamp', how='left')\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['year','week_day','month','week']\n",
    "harmonic_features = ['week_day','month','week']\n",
    "harmonic_features_max = [6, 12, 53]\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "def combine_y_hats(y_hats):\n",
    "    X_stack = pd.DataFrame({})\n",
    "    for i in range(0, len(y_hats)):\n",
    "        X_stack['stack'+str(i)] = y_hats[i]\n",
    "    return X_stack\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "\n",
    "def validate_stack(X_stack, y, model):\n",
    "    y_cros_val_pred = cross_val_predict(model, X_stack, y=y, cv=5, n_jobs=-1)\n",
    "    return y_cros_val_pred\n",
    "\n",
    "def cv_lr(X_stack, y, model):\n",
    "    \n",
    "    # cros val lasso on X_stack \n",
    "    \n",
    "    y_cros_val_pred = pd.DataFrame({})\n",
    "    coefs = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_stack):\n",
    "        X_train, X_test = X_stack.iloc[train_index], X_stack.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_temp = model \n",
    "        model_temp.fit(X_train,y_train)\n",
    "        y_hat = model_temp.predict(X_test)\n",
    "        coefs.append(model_temp.coef_)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'id': test_index, 'Value': y_hat})\n",
    "        y_cros_val_pred = pd.concat([y_cros_val_pred, temp_df], axis=0)\n",
    "      \n",
    "    y_cros_val_pred = y_cros_val_pred.sort_values(by='id')\n",
    "    \n",
    "    #divide by sum - due to overfit of model, with dividing - coefs ~ weights of models\n",
    "    return list(y_cros_val_pred['Value']) / np.sum(np.mean(coefs,axis=0)) \n",
    "\n",
    "def make_harmonic_features(x, col, period=24):\n",
    "    x['sin_'+col] = np.sin(x[col] * 2 * np.pi / period)\n",
    "    x['cos_'+col] = np.cos(x[col] * 2 * np.pi / period)\n",
    "    x = x.drop(col, axis=1)\n",
    "    return x\n",
    "\n",
    "index_mult = 0\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[0:50]:\n",
    "    lr_stack = Lasso(alpha=1, fit_intercept=False, max_iter=3000, tol=0.0001, positive=True, random_state=424142)\n",
    "    \n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 9999\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    up_border = X_train['Value'].quantile(0.985)\n",
    "    low_border = X_train['Value'].quantile(0.015)\n",
    "    X_train = X_train[(X_train['Value']<=up_border) & (X_train['Value']>=low_border)].reset_index(drop=True)\n",
    "    \n",
    "    X_train['Value'] = abs(X_train['Value']) \n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "     \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    #X_test = pd.merge(X_test, hol, on='Timestamp', how='left')\n",
    "    #X_test = pd.merge(X_test, h, on='Timestamp', how='left')\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe') \n",
    "    X_train_harm, X_test_harm = feature_preprocessing(X_train, X_test, harmonic_features, cat_type='harmonic')\n",
    "  \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    X_train_v_harm, X_valid_v_harm = feature_preprocessing(X_train_v, X_valid_v, harmonic_features, cat_type='harmonic')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_harm = train_rf(X_train_v_harm, X_train_v['Value'], X_valid_v_harm)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    y_rf_log_harm = np.exp(train_rf(X_train_v_harm, X_train_v['Value_log'], X_valid_v_harm)) - 1\n",
    "    \n",
    "    X_valid_stack = combine_y_hats([y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                                    y_hat_grby_median, y_hat_grby3_median, \n",
    "                                    y_rf, y_rf_harm, \n",
    "                                    y_rf_log, y_rf_log_harm, \n",
    "                                    \n",
    "                                   # y_lgb, y_lgb_mean, \n",
    "                                   # y_lgb_log, y_lgb_mean_log\n",
    "                                   ])\n",
    "    y_lr_hat = cv_lr(X_valid_stack, X_valid_v['Value'], lr_stack)\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], 0)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], 0)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], 0)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], 0)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_harm = calc_score(y_rf_harm, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log_harm = calc_score(y_rf_log_harm, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_lr_stack = calc_score(y_lr_hat, X_valid_v['Value'], index_mult)\n",
    "\n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, y_rf_harm,\n",
    "                  y_rf_log, y_rf_log_harm,\n",
    "                  y_lr_hat\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, score_rf_harm,\n",
    "                  score_rf_log, score_rf_log_harm,\n",
    "                  score_lr_stack\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_harm',\n",
    "                    'rf_log', 'rf_log_harm',\n",
    "                    'lr_stack'\n",
    "                   ]\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "\n",
    "    if (score_rf==best_score):\n",
    "        y_hat = train_rf(X_train_ohe, X_train['Value'], X_test_ohe)\n",
    "        \n",
    "    if (score_rf_log==best_score):\n",
    "        y_hat = np.exp(train_rf(X_train_ohe, X_train['Value_log'], X_test_ohe)) -1    \n",
    "        \n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    # print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "sub = sub[['Timestamp', 'ForecastId', 'Value']]\n",
    "sub.columns = ['DATE', 'ATM_ID', 'CLIENT_OUT']\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with days to holiday \n",
    "\n",
    "\n",
    "# with holidays \n",
    "60 Best model is mean_all\n",
    "val score: 4139.33460426 + 2082.87253026\n",
    "{'mean_all': 4301.028429876416, 'rf_mean': 4383.672882474736, 'median_all': 4595.50858652576, \n",
    " 'rf': 4784.1237340378675, 'mean_3': 5520.169214317167, 'median_3': 6169.4187582562745}\n",
    "\n",
    "\n",
    "# no holidays \n",
    "60 Best model is mean_all\n",
    "val score: 4086.31176171 + 1982.26470997\n",
    "{'mean_all': 4301.028429876416, 'rf_mean': 4306.659144071697, 'median_all': 4595.50858652576, 'rf': 4675.956993982094, 'mean_3': 5520.169214317167, 'median_3': 6169.4187582562745}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "419 Best model is mean_3\n",
    "val score: 3935.81800742 + 1661.80843643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "#hol = pd.read_csv('hol.csv')\n",
    "#h = pd.read_csv('golds.csv')\n",
    "#hol['Timestamp'] = pd.to_datetime(hol['Timestamp'])\n",
    "#h['Timestamp'] = pd.to_datetime(h['Timestamp'])\n",
    "\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    return X\n",
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i+'_'+j], X_test[i+'_'+j] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    if (cat_type=='harmonic'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        temp_train = ohe.fit_transform(train[['year']])\n",
    "        temp_test = ohe.transform(test[['year']])\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(temp_train)],axis=1)\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(temp_test)],axis=1)\n",
    "        for i in range(len(cat_cols)):\n",
    "            X_train = make_harmonic_features(X_train, cat_cols[i], period=harmonic_features_max[i])\n",
    "            X_test = make_harmonic_features(X_test, cat_cols[i], period=harmonic_features_max[i])\n",
    "        \n",
    "    #X_train = pd.concat([X_train, train[new_cols]], axis=1).fillna(-999)\n",
    "    #X_test = pd.concat([X_test, test[new_cols]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test \n",
    "\n",
    "def train_groupby(train, test, window, how):\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=180, n_jobs=-1, random_state=4224, criterion='mae')\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "train = time_preprocess(train)\n",
    "print(train.shape)\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['year','week_day','month','week']\n",
    "harmonic_features = ['week_day','month','week']\n",
    "harmonic_features_max = [6, 12, 53]\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "def combine_y_hats(y_hats):\n",
    "    X_stack = pd.DataFrame({})\n",
    "    for i in range(0, len(y_hats)):\n",
    "        X_stack['stack'+str(i)] = y_hats[i]\n",
    "    return X_stack\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "\n",
    "def validate_stack(X_stack, y, model):\n",
    "    y_cros_val_pred = cross_val_predict(model, X_stack, y=y, cv=5, n_jobs=-1)\n",
    "    return y_cros_val_pred\n",
    "\n",
    "def validate_lgb(X_train, y_train, X_valid, y_valid):\n",
    "    \n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    d2 = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        #'min_data':1, 'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, valid_sets=d2, \n",
    "                    num_boost_round=50000, early_stopping_rounds=100)\n",
    "    \n",
    "    y_hat = gbm.predict(X_valid)\n",
    "    opt_boost_rounds = gbm.best_iteration\n",
    "    \n",
    "    return y_hat, opt_boost_rounds \n",
    "\n",
    "\n",
    "\n",
    "def train_lgb(X_train, y_train, X_test, opt_boost_rounds):\n",
    "    \n",
    "    d1 = lgb.Dataset(X_train, y_train, weight=np.linspace(0.5, 1, X_train.shape[0]))\n",
    "    \n",
    "    params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.160042,\n",
    "        'random_state':42,\n",
    "        #'min_data':1, 'min_data_in_bin':1\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params, d1, verbose_eval=None, num_boost_round=opt_boost_rounds)\n",
    "    \n",
    "    y_hat = gbm.predict(X_test)\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "def MeanEncodingTransforming(X, y, X_test, how_to_fill):\n",
    "    \n",
    "    # mean encoding for lgb\n",
    "    \n",
    "    X_train = pd.concat([X, y], axis=1)\n",
    "    mean_values = X_train.groupby(X_train.columns[0]).agg(how_to_fill).to_dict()['Value']\n",
    "    X_train = X_train.drop(y.columns[0], axis=1)\n",
    "    X_train = X_train.replace(mean_values)\n",
    "    X_test = X_test.replace(mean_values)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def cv_lr(X_stack, y, model):\n",
    "    \n",
    "    # cros val lasso on X_stack \n",
    "    \n",
    "    y_cros_val_pred = pd.DataFrame({})\n",
    "    coefs = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_stack):\n",
    "        X_train, X_test = X_stack.iloc[train_index], X_stack.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model_temp = model \n",
    "        model_temp.fit(X_train,y_train)\n",
    "        y_hat = model_temp.predict(X_test)\n",
    "        coefs.append(model_temp.coef_)\n",
    "        \n",
    "        temp_df = pd.DataFrame({'id': test_index, 'Value': y_hat})\n",
    "        y_cros_val_pred = pd.concat([y_cros_val_pred, temp_df], axis=0)\n",
    "      \n",
    "    y_cros_val_pred = y_cros_val_pred.sort_values(by='id')\n",
    "    \n",
    "    #divide by sum - due to overfit of model, with dividing - coefs ~ weights of models\n",
    "    return list(y_cros_val_pred['Value']) / np.sum(np.mean(coefs,axis=0)) \n",
    "\n",
    "def make_harmonic_features(x, col, period=24):\n",
    "    x['sin_'+col] = np.sin(x[col] * 2 * np.pi / period)\n",
    "    x['cos_'+col] = np.cos(x[col] * 2 * np.pi / period)\n",
    "    x = x.drop(col, axis=1)\n",
    "    return x\n",
    "\n",
    "index_mult = 0\n",
    "\n",
    "for i in pd.unique(train['ForecastId'])[0:50]:\n",
    "    lr_stack = Lasso(alpha=1, fit_intercept=False, max_iter=3000, tol=0.0001, positive=True, random_state=424142)\n",
    "    \n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 9999\n",
    "    X_train = train[train['ForecastId']==i].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    up_border = X_train['Value'].quantile(0.985)\n",
    "    low_border = X_train['Value'].quantile(0.015)\n",
    "    X_train = X_train[(X_train['Value']<=up_border) & (X_train['Value']>=low_border)].reset_index(drop=True)\n",
    "    \n",
    "    X_train['Value'] = abs(X_train['Value']) \n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "     \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    #X_test = pd.merge(X_test, hol, on='Timestamp', how='left')\n",
    "    #X_test = pd.merge(X_test, h, on='Timestamp', how='left')\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe') \n",
    "    X_train_meanenc, X_test_meanenc = feature_preprocessing(X_train, X_test, cat_cols, cat_type='mean_enc')\n",
    "    X_train_harm, X_test_harm = feature_preprocessing(X_train, X_test, harmonic_features, cat_type='harmonic')\n",
    "  \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    X_train_v_meanenc, X_valid_v_meanenc = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='mean_enc')\n",
    "    X_train_v_harm, X_valid_v_harm = feature_preprocessing(X_train_v, X_valid_v, harmonic_features, cat_type='harmonic')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_harm = train_rf(X_train_v_harm, X_train_v['Value'], X_valid_v_harm)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    y_rf_log_harm = np.exp(train_rf(X_train_v_harm, X_train_v['Value_log'], X_valid_v_harm)) - 1\n",
    "    \n",
    "        # LightGBM\n",
    "    y_lgb, lgb_opt = validate_lgb(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe, X_valid_v['Value'])\n",
    "    y_lgb_mean, lgb_mean_opt = validate_lgb(X_train_v_meanenc, X_train_v['Value'], X_valid_v_meanenc, X_valid_v['Value'])\n",
    "    y_lgb_log, lgb_opt_log = validate_lgb(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe, X_valid_v['Value_log'])\n",
    "    y_lgb_log = np.exp(y_lgb_log) -1\n",
    "    y_lgb_mean_log, lgb_mean_opt_log = validate_lgb(X_train_v_meanenc, X_train_v['Value_log'], X_valid_v_meanenc, X_valid_v['Value_log'])\n",
    "    y_lgb_mean_log = np.exp(y_lgb_mean_log) -1\n",
    "    \n",
    "    \n",
    "    X_valid_stack = combine_y_hats([y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                                    y_hat_grby_median, y_hat_grby3_median, \n",
    "                                    y_rf, y_rf_harm, \n",
    "                                    y_rf_log, y_rf_log_harm, \n",
    "                                   y_lgb, y_lgb_mean, \n",
    "                                   y_lgb_log, y_lgb_mean_log\n",
    "                                   ])\n",
    "    y_lr_hat = cv_lr(X_valid_stack, X_valid_v['Value'], lr_stack)\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], 0)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], 0)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], 0)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], 0)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    score_lgb = calc_score(y_lgb, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_mean = calc_score(y_lgb_mean, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_log = calc_score(y_lgb_log, X_valid_v['Value'], index_mult)\n",
    "    score_lgb_mean_log = calc_score(y_lgb_mean_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    score_lr_stack = calc_score(y_lr_hat, X_valid_v['Value'], index_mult)\n",
    "\n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, \n",
    "                  y_rf_log, \n",
    "                  y_lgb, y_lgb_mean, y_lgb_log, y_lgb_mean_log,\n",
    "                  y_lr_hat\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, \n",
    "                  score_rf_log, \n",
    "                  score_lgb, score_lgb_mean, score_lgb_log, score_lgb_mean_log,\n",
    "                  score_lr_stack\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', \n",
    "                    'rf_log', \n",
    "                    'score_lgb', 'score_lgb_mean', 'score_lgb_log', 'score_lgb_mean_log',\n",
    "                    'lr_stack'\n",
    "                   ]\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "\n",
    "    if (score_rf==best_score):\n",
    "        y_hat = train_rf(X_train_ohe, X_train['Value'], X_test_ohe)\n",
    "        \n",
    "    if (score_rf_log==best_score):\n",
    "        y_hat = np.exp(train_rf(X_train_ohe, X_train['Value_log'], X_test_ohe)) -1    \n",
    "    \n",
    "    y_hat = 0\n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "  \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    # print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "sub = sub[['Timestamp', 'ForecastId', 'Value']]\n",
    "sub.columns = ['DATE', 'ATM_ID', 'CLIENT_OUT']\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\pandas\\core\\groupby.py:707: FutureWarning: pd.rolling_sum is deprecated for DataFrame and will be removed in a future version, replace with \n",
      "\tDataFrame.rolling(window=7,center=False,min_periods=1).sum()\n",
      "  return func(g, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177900, 10)\n",
      "(167834, 8)\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[50]\tvalid_0's l1: 346088\tvalid_0's rmsle: 8473.79\n",
      "[100]\tvalid_0's l1: 342455\tvalid_0's rmsle: 8384.84\n",
      "[150]\tvalid_0's l1: 338878\tvalid_0's rmsle: 8297.26\n",
      "[200]\tvalid_0's l1: 335281\tvalid_0's rmsle: 8209.18\n",
      "[250]\tvalid_0's l1: 331729\tvalid_0's rmsle: 8122.22\n",
      "[300]\tvalid_0's l1: 328186\tvalid_0's rmsle: 8035.47\n",
      "[350]\tvalid_0's l1: 324793\tvalid_0's rmsle: 7952.39\n",
      "[400]\tvalid_0's l1: 321348\tvalid_0's rmsle: 7868.04\n",
      "[450]\tvalid_0's l1: 317877\tvalid_0's rmsle: 7783.06\n",
      "[500]\tvalid_0's l1: 314546\tvalid_0's rmsle: 7701.49\n",
      "[550]\tvalid_0's l1: 311312\tvalid_0's rmsle: 7622.31\n",
      "[600]\tvalid_0's l1: 308040\tvalid_0's rmsle: 7542.2\n",
      "[650]\tvalid_0's l1: 304818\tvalid_0's rmsle: 7463.32\n",
      "[700]\tvalid_0's l1: 301549\tvalid_0's rmsle: 7383.28\n",
      "[750]\tvalid_0's l1: 298303\tvalid_0's rmsle: 7303.81\n",
      "[800]\tvalid_0's l1: 295056\tvalid_0's rmsle: 7224.31\n",
      "[850]\tvalid_0's l1: 291945\tvalid_0's rmsle: 7148.13\n",
      "[900]\tvalid_0's l1: 288777\tvalid_0's rmsle: 7070.55\n",
      "[950]\tvalid_0's l1: 285680\tvalid_0's rmsle: 6994.73\n",
      "[1000]\tvalid_0's l1: 282529\tvalid_0's rmsle: 6917.57\n",
      "[1050]\tvalid_0's l1: 279487\tvalid_0's rmsle: 6843.09\n",
      "[1100]\tvalid_0's l1: 276530\tvalid_0's rmsle: 6770.69\n",
      "[1150]\tvalid_0's l1: 273580\tvalid_0's rmsle: 6698.46\n",
      "[1200]\tvalid_0's l1: 270699\tvalid_0's rmsle: 6627.92\n",
      "[1250]\tvalid_0's l1: 267831\tvalid_0's rmsle: 6557.72\n",
      "[1300]\tvalid_0's l1: 264980\tvalid_0's rmsle: 6487.91\n",
      "[1350]\tvalid_0's l1: 262137\tvalid_0's rmsle: 6418.29\n",
      "[1400]\tvalid_0's l1: 259418\tvalid_0's rmsle: 6351.71\n",
      "[1450]\tvalid_0's l1: 256763\tvalid_0's rmsle: 6286.71\n",
      "[1500]\tvalid_0's l1: 254108\tvalid_0's rmsle: 6221.7\n",
      "[1550]\tvalid_0's l1: 251459\tvalid_0's rmsle: 6156.85\n",
      "[1600]\tvalid_0's l1: 248742\tvalid_0's rmsle: 6090.33\n",
      "[1650]\tvalid_0's l1: 246176\tvalid_0's rmsle: 6027.49\n",
      "[1700]\tvalid_0's l1: 243889\tvalid_0's rmsle: 5971.5\n",
      "[1750]\tvalid_0's l1: 241369\tvalid_0's rmsle: 5909.81\n",
      "[1800]\tvalid_0's l1: 238860\tvalid_0's rmsle: 5848.35\n",
      "[1850]\tvalid_0's l1: 236446\tvalid_0's rmsle: 5789.25\n",
      "[1900]\tvalid_0's l1: 234121\tvalid_0's rmsle: 5732.34\n",
      "[1950]\tvalid_0's l1: 231823\tvalid_0's rmsle: 5676.06\n",
      "[2000]\tvalid_0's l1: 229534\tvalid_0's rmsle: 5620.02\n",
      "[2050]\tvalid_0's l1: 227293\tvalid_0's rmsle: 5565.15\n",
      "[2100]\tvalid_0's l1: 225107\tvalid_0's rmsle: 5511.63\n",
      "[2150]\tvalid_0's l1: 222920\tvalid_0's rmsle: 5458.08\n",
      "[2200]\tvalid_0's l1: 220739\tvalid_0's rmsle: 5404.68\n",
      "[2250]\tvalid_0's l1: 218662\tvalid_0's rmsle: 5353.84\n",
      "[2300]\tvalid_0's l1: 216591\tvalid_0's rmsle: 5303.11\n",
      "[2350]\tvalid_0's l1: 214710\tvalid_0's rmsle: 5257.07\n",
      "[2400]\tvalid_0's l1: 212730\tvalid_0's rmsle: 5208.58\n",
      "[2450]\tvalid_0's l1: 210779\tvalid_0's rmsle: 5160.82\n",
      "[2500]\tvalid_0's l1: 208975\tvalid_0's rmsle: 5116.65\n",
      "[2550]\tvalid_0's l1: 207102\tvalid_0's rmsle: 5070.79\n",
      "[2600]\tvalid_0's l1: 205232\tvalid_0's rmsle: 5025\n",
      "[2650]\tvalid_0's l1: 203302\tvalid_0's rmsle: 4977.75\n",
      "[2700]\tvalid_0's l1: 201538\tvalid_0's rmsle: 4934.56\n",
      "[2750]\tvalid_0's l1: 199761\tvalid_0's rmsle: 4891.05\n",
      "[2800]\tvalid_0's l1: 198073\tvalid_0's rmsle: 4849.71\n",
      "[2850]\tvalid_0's l1: 196267\tvalid_0's rmsle: 4805.5\n",
      "[2900]\tvalid_0's l1: 194642\tvalid_0's rmsle: 4765.7\n",
      "[2950]\tvalid_0's l1: 193059\tvalid_0's rmsle: 4726.95\n",
      "[3000]\tvalid_0's l1: 191505\tvalid_0's rmsle: 4688.89\n",
      "[3050]\tvalid_0's l1: 189950\tvalid_0's rmsle: 4650.84\n",
      "[3100]\tvalid_0's l1: 188502\tvalid_0's rmsle: 4615.37\n",
      "[3150]\tvalid_0's l1: 187065\tvalid_0's rmsle: 4580.2\n",
      "[3200]\tvalid_0's l1: 185807\tvalid_0's rmsle: 4549.38\n",
      "[3250]\tvalid_0's l1: 184519\tvalid_0's rmsle: 4517.86\n",
      "[3300]\tvalid_0's l1: 183418\tvalid_0's rmsle: 4490.91\n",
      "[3350]\tvalid_0's l1: 182357\tvalid_0's rmsle: 4464.91\n",
      "[3400]\tvalid_0's l1: 181379\tvalid_0's rmsle: 4440.96\n",
      "[3450]\tvalid_0's l1: 180360\tvalid_0's rmsle: 4416.03\n",
      "[3500]\tvalid_0's l1: 179330\tvalid_0's rmsle: 4390.79\n",
      "[3550]\tvalid_0's l1: 178310\tvalid_0's rmsle: 4365.84\n",
      "[3600]\tvalid_0's l1: 177727\tvalid_0's rmsle: 4351.55\n",
      "[3650]\tvalid_0's l1: 177193\tvalid_0's rmsle: 4338.48\n",
      "[3700]\tvalid_0's l1: 176684\tvalid_0's rmsle: 4326.03\n",
      "[3750]\tvalid_0's l1: 176231\tvalid_0's rmsle: 4314.92\n",
      "[3800]\tvalid_0's l1: 175883\tvalid_0's rmsle: 4306.41\n",
      "[3850]\tvalid_0's l1: 175507\tvalid_0's rmsle: 4297.2\n",
      "[3900]\tvalid_0's l1: 175009\tvalid_0's rmsle: 4285\n",
      "[3950]\tvalid_0's l1: 174656\tvalid_0's rmsle: 4276.36\n",
      "[4000]\tvalid_0's l1: 174256\tvalid_0's rmsle: 4266.57\n",
      "[4050]\tvalid_0's l1: 173737\tvalid_0's rmsle: 4253.87\n",
      "[4100]\tvalid_0's l1: 173334\tvalid_0's rmsle: 4243.99\n",
      "[4150]\tvalid_0's l1: 173091\tvalid_0's rmsle: 4238.04\n",
      "[4200]\tvalid_0's l1: 172937\tvalid_0's rmsle: 4234.27\n",
      "[4250]\tvalid_0's l1: 172464\tvalid_0's rmsle: 4222.69\n",
      "[4300]\tvalid_0's l1: 172177\tvalid_0's rmsle: 4215.66\n",
      "[4350]\tvalid_0's l1: 171751\tvalid_0's rmsle: 4205.24\n",
      "[4400]\tvalid_0's l1: 171515\tvalid_0's rmsle: 4199.46\n",
      "[4450]\tvalid_0's l1: 171174\tvalid_0's rmsle: 4191.11\n",
      "[4500]\tvalid_0's l1: 170865\tvalid_0's rmsle: 4183.54\n",
      "[4550]\tvalid_0's l1: 170562\tvalid_0's rmsle: 4176.12\n",
      "[4600]\tvalid_0's l1: 170194\tvalid_0's rmsle: 4167.11\n",
      "[4650]\tvalid_0's l1: 169844\tvalid_0's rmsle: 4158.55\n",
      "[4700]\tvalid_0's l1: 169530\tvalid_0's rmsle: 4150.86\n",
      "[4750]\tvalid_0's l1: 169221\tvalid_0's rmsle: 4143.29\n",
      "[4800]\tvalid_0's l1: 168906\tvalid_0's rmsle: 4135.58\n",
      "[4850]\tvalid_0's l1: 168531\tvalid_0's rmsle: 4126.4\n",
      "[4900]\tvalid_0's l1: 168383\tvalid_0's rmsle: 4122.78\n",
      "[4950]\tvalid_0's l1: 168125\tvalid_0's rmsle: 4116.45\n",
      "[5000]\tvalid_0's l1: 167865\tvalid_0's rmsle: 4110.09\n",
      "[5050]\tvalid_0's l1: 167561\tvalid_0's rmsle: 4102.64\n",
      "[5100]\tvalid_0's l1: 167305\tvalid_0's rmsle: 4096.37\n",
      "[5150]\tvalid_0's l1: 167070\tvalid_0's rmsle: 4090.63\n",
      "[5200]\tvalid_0's l1: 166629\tvalid_0's rmsle: 4079.83\n",
      "[5250]\tvalid_0's l1: 166305\tvalid_0's rmsle: 4071.89\n",
      "[5300]\tvalid_0's l1: 166009\tvalid_0's rmsle: 4064.64\n",
      "[5350]\tvalid_0's l1: 165702\tvalid_0's rmsle: 4057.13\n",
      "[5400]\tvalid_0's l1: 165545\tvalid_0's rmsle: 4053.29\n",
      "[5450]\tvalid_0's l1: 165361\tvalid_0's rmsle: 4048.79\n",
      "[5500]\tvalid_0's l1: 165135\tvalid_0's rmsle: 4043.25\n",
      "[5550]\tvalid_0's l1: 164929\tvalid_0's rmsle: 4038.2\n",
      "[5600]\tvalid_0's l1: 164692\tvalid_0's rmsle: 4032.4\n",
      "[5650]\tvalid_0's l1: 164442\tvalid_0's rmsle: 4026.28\n",
      "[5700]\tvalid_0's l1: 164303\tvalid_0's rmsle: 4022.87\n",
      "[5750]\tvalid_0's l1: 164086\tvalid_0's rmsle: 4017.56\n",
      "[5800]\tvalid_0's l1: 163907\tvalid_0's rmsle: 4013.17\n",
      "[5850]\tvalid_0's l1: 163881\tvalid_0's rmsle: 4012.55\n",
      "[5900]\tvalid_0's l1: 163736\tvalid_0's rmsle: 4008.99\n",
      "[5950]\tvalid_0's l1: 163570\tvalid_0's rmsle: 4004.93\n",
      "[6000]\tvalid_0's l1: 163367\tvalid_0's rmsle: 3999.95\n",
      "[6050]\tvalid_0's l1: 163178\tvalid_0's rmsle: 3995.32\n",
      "[6100]\tvalid_0's l1: 163005\tvalid_0's rmsle: 3991.1\n",
      "[6150]\tvalid_0's l1: 162840\tvalid_0's rmsle: 3987.07\n",
      "[6200]\tvalid_0's l1: 162635\tvalid_0's rmsle: 3982.03\n",
      "[6250]\tvalid_0's l1: 162460\tvalid_0's rmsle: 3977.76\n",
      "[6300]\tvalid_0's l1: 162222\tvalid_0's rmsle: 3971.92\n",
      "[6350]\tvalid_0's l1: 162115\tvalid_0's rmsle: 3969.3\n",
      "[6400]\tvalid_0's l1: 162006\tvalid_0's rmsle: 3966.62\n",
      "[6450]\tvalid_0's l1: 161901\tvalid_0's rmsle: 3964.07\n",
      "[6500]\tvalid_0's l1: 161761\tvalid_0's rmsle: 3960.63\n",
      "[6550]\tvalid_0's l1: 161507\tvalid_0's rmsle: 3954.42\n",
      "[6600]\tvalid_0's l1: 161410\tvalid_0's rmsle: 3952.05\n",
      "[6650]\tvalid_0's l1: 161301\tvalid_0's rmsle: 3949.37\n",
      "[6700]\tvalid_0's l1: 161170\tvalid_0's rmsle: 3946.16\n",
      "[6750]\tvalid_0's l1: 161065\tvalid_0's rmsle: 3943.6\n",
      "[6800]\tvalid_0's l1: 160961\tvalid_0's rmsle: 3941.04\n",
      "[6850]\tvalid_0's l1: 160876\tvalid_0's rmsle: 3938.97\n",
      "[6900]\tvalid_0's l1: 160764\tvalid_0's rmsle: 3936.22\n",
      "[6950]\tvalid_0's l1: 160641\tvalid_0's rmsle: 3933.21\n",
      "[7000]\tvalid_0's l1: 160650\tvalid_0's rmsle: 3933.43\n",
      "[7050]\tvalid_0's l1: 160583\tvalid_0's rmsle: 3931.78\n",
      "[7100]\tvalid_0's l1: 160491\tvalid_0's rmsle: 3929.53\n",
      "[7150]\tvalid_0's l1: 160356\tvalid_0's rmsle: 3926.23\n",
      "[7200]\tvalid_0's l1: 160287\tvalid_0's rmsle: 3924.54\n",
      "[7250]\tvalid_0's l1: 160200\tvalid_0's rmsle: 3922.41\n",
      "[7300]\tvalid_0's l1: 160062\tvalid_0's rmsle: 3919.04\n",
      "[7350]\tvalid_0's l1: 160019\tvalid_0's rmsle: 3917.97\n",
      "[7400]\tvalid_0's l1: 159910\tvalid_0's rmsle: 3915.31\n",
      "[7450]\tvalid_0's l1: 159874\tvalid_0's rmsle: 3914.43\n",
      "[7500]\tvalid_0's l1: 159808\tvalid_0's rmsle: 3912.82\n",
      "[7550]\tvalid_0's l1: 159631\tvalid_0's rmsle: 3908.49\n",
      "[7600]\tvalid_0's l1: 159529\tvalid_0's rmsle: 3906\n",
      "[7650]\tvalid_0's l1: 159464\tvalid_0's rmsle: 3904.39\n",
      "[7700]\tvalid_0's l1: 159386\tvalid_0's rmsle: 3902.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7750]\tvalid_0's l1: 159258\tvalid_0's rmsle: 3899.34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    197\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2018\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1437\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1438\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import lightgbm as lgb\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['week'] = X['Timestamp'].dt.week\n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    return X\n",
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "train = time_preprocess(train)\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "\n",
    "val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "val['Value'] = val['Value'].fillna(0)\n",
    "val['Value'] = (val['Value']==0) * 1\n",
    "val.columns = ['diff']\n",
    "train = pd.concat([train, val], axis=1)\n",
    "\n",
    "a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "a.columns = ['ForecastId', 'to_drop']\n",
    "a = a['to_drop']\n",
    "train = pd.concat([train, a], axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "train = train[['Timestamp','ForecastId','Value']]\n",
    "train = time_preprocess(train)\n",
    "print(train.shape)\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "train_mask = train.Timestamp<=pd.to_datetime('2017-07-16')\n",
    "valid_mask = train.Timestamp>pd.to_datetime('2017-07-16')\n",
    "\n",
    "def MeanEncodingTransforming(X, y, X_test, how_to_fill):\n",
    "    X_train = pd.concat([X, y], axis=1)\n",
    "    mean_values = X_train.groupby(X_train.columns[0]).agg(how_to_fill).to_dict()['Value']\n",
    "    X_train = X_train.drop(y.columns[0], axis=1)\n",
    "    X_train = X_train.replace(mean_values)\n",
    "    X_test = X_test.replace(mean_values)\n",
    "    return X_train, X_test\n",
    "\n",
    "cat_cols = ['ForecastId','year','week_day','month','week','day']\n",
    "X_train = train[['ForecastId', 'year', 'month','week','day','week_day','Value']][train_mask]\n",
    "X_valid = train[['ForecastId', 'year', 'month','week','day','week_day','Value']][valid_mask]\n",
    "y_train = train['Value'][train_mask]\n",
    "y_valid = train['Value'][valid_mask]\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    # ohe or mean encoding preprocessing for the lgb \n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i+'_'+j], X_test[i+'_'+j] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        \n",
    "    if (cat_type=='harmonic'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        temp_train = ohe.fit_transform(train[['year']])\n",
    "        temp_test = ohe.transform(test[['year']])\n",
    "        X_train = pd.concat([X_train, pd.DataFrame(temp_train)],axis=1)\n",
    "        X_test = pd.concat([X_test, pd.DataFrame(temp_test)],axis=1)\n",
    "        for i in range(len(cat_cols)):\n",
    "            X_train = make_harmonic_features(X_train, cat_cols[i], period=harmonic_features_max[i])\n",
    "            X_test = make_harmonic_features(X_test, cat_cols[i], period=harmonic_features_max[i])\n",
    "        \n",
    "    #X_train = pd.concat([X_train, train[new_cols]], axis=1).fillna(-999)\n",
    "    #X_test = pd.concat([X_test, test[new_cols]], axis=1).fillna(-999)\n",
    "    \n",
    "    return X_train, X_test \n",
    "\n",
    "max_values = train[['Value', 'ForecastId']].groupby('ForecastId').agg('max').reset_index()\n",
    "max_values.columns = ['ForecastId', 'max_value']\n",
    "train1 = train.copy()\n",
    "train1 = pd.merge(train1, max_values, on = 'ForecastId', how='left')\n",
    "train1['Value'] = train1['Value'] / train1['max_value']\n",
    "train1 = train1.drop('max_value',axis=1)\n",
    "train1['year_month_weekday'] = train1['year'].astype('str') + '_' + train1['month'].astype('str') + '_' + train1['week_day'].astype('str') \n",
    "features = train1[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "features = features.fillna(0).reset_index()\n",
    "\n",
    "X_train, X_valid = feature_preprocessing(X_train, X_valid, cat_cols, cat_type='mean_enc')\n",
    "X_train = pd.merge(X_train, features, how='left', on='ForecastId')\n",
    "X_valid = pd.merge(X_valid, features, how='left', on='ForecastId')\n",
    "\n",
    "# custom metric \n",
    "def lgb_rmsle_score(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = preds\n",
    "    return 'rmsle', np.sum(abs(labels-preds))/np.sum(labels)*10000, False\n",
    "\n",
    "dtrain = lgb.Dataset(X_train,y_train)\n",
    "dval = lgb.Dataset(X_valid,y_valid)\n",
    "\n",
    "\n",
    "\n",
    "#dtrain_all = lgb.Dataset(train[['ForecastId', 'year', 'month','week','day','week_day']],\n",
    "#                         train['Value'],\n",
    "#                         categorical_feature=['ForecastId', 'year', 'month','week','day','week_day'])\n",
    "\n",
    "params = {\n",
    "        'objective':'regression',    \n",
    "        'metric': 'l1', \n",
    "        'learning_rate': 0.050042,\n",
    "        'random_state':42,\n",
    "        #'min_data':1, 'min_data_in_bin':1\n",
    "    }\n",
    "\n",
    "xgb = lgb.train(params, dtrain, valid_sets=dval, feval=lgb_rmsle_score, \n",
    "              verbose_eval=50, num_boost_round=1999999, early_stopping_rounds=150)\n",
    "\n",
    "#xgb = lgb.train(params, dtrain_all, \n",
    "#                num_boost_round=xgb.best_iteration)\n",
    "\n",
    "#X_test = pd.DataFrame({'Timestamp':pred_dates})\n",
    "#X_test = time_preprocess(X_test)\n",
    "\n",
    "#temp = pd.DataFrame({})\n",
    "#for iid in pd.unique(train.ForecastId)[0:50]:\n",
    "#    test_temp = X_test[['year', 'month','week','day','week_day','Timestamp']]\n",
    "#    test_temp['ForecastId'] = iid\n",
    "#    test_temp['Value'] = xgb.predict(test_temp[['ForecastId', 'year', 'month','week','day','week_day']])\n",
    "#    temp = pd.concat([temp, test_temp],axis=0)\n",
    "#    \n",
    "#temp = temp[['Timestamp','ForecastId','Value']]\n",
    "#temp.columns = ['DATE', 'ATM_ID', 'CLIENT_OUT']\n",
    "#temp.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
