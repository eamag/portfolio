{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T18:27:58.506202Z",
     "start_time": "2018-04-03T18:27:58.492105Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-03T18:52:13.061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eamag/.pyenv/versions/3.6.0/envs/general/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109800, 9)\n",
      "0 Best model is mean_3\n",
      "loss: 1911.202821592585\n",
      "val score: 1911.202821592585 + 0.0\n",
      "{'mean_all': 1929.6019321937838, 'mean_3': 1911.202821592585, 'median_all': 1917.2474893321148, 'median_3': 2051.5422874413334, 'rf': 2081.5226097269688, 'rf_mean': 1948.266932700504}\n",
      "2 Best model is rf_mean\n",
      "loss: 7308.297692734063\n",
      "val score: 4609.750257163324 + 2698.5474355707393\n",
      "{'mean_all': 11576.547508758478, 'mean_3': 11281.01885868234, 'median_all': 12067.597354886113, 'median_3': 12468.772961058046, 'rf': 13692.97979747855, 'rf_mean': 7308.297692734063}\n",
      "3 Best model is mean_all\n",
      "loss: 5371.368878734829\n",
      "val score: 4863.623131020492 + 2232.414622520782\n",
      "{'mean_all': 5371.368878734829, 'mean_3': 5446.544910819182, 'median_all': 5795.151126775606, 'median_3': 5647.12164904411, 'rf': 6588.766953271631, 'rf_mean': 5694.037977872376}\n",
      "4 Best model is mean_3\n",
      "loss: 4056.988259920811\n",
      "val score: 4661.964413245572 + 1964.6259187176902\n",
      "{'mean_all': 4573.25915450995, 'mean_3': 4056.988259920811, 'median_all': 4285.73121241743, 'median_3': 4062.9757990461803, 'rf': 4783.187884317709, 'rf_mean': 8365.955196273617}\n",
      "5 Best model is median_3\n",
      "loss: 4480.163054445655\n",
      "val score: 4625.604141485589 + 1758.718930883415\n",
      "{'mean_all': 6182.502999216782, 'mean_3': 5285.035853143371, 'median_all': 5342.404245873189, 'median_3': 4480.163054445655, 'rf': 6494.8942631889195, 'rf_mean': 8371.132708923633}\n",
      "6 Best model is median_all\n",
      "loss: 1543.660676654278\n",
      "val score: 4111.9468973470375 + 1974.0303241050976\n",
      "{'mean_all': 1731.2067317810502, 'mean_3': 1932.9041981174248, 'median_all': 1543.660676654278, 'median_3': 1602.7434001302731, 'rf': 1738.5277460179796, 'rf_mean': 2311.103481401945}\n",
      "7 Best model is rf_mean\n",
      "loss: 6623.286221745721\n",
      "val score: 4470.709657975421 + 2027.8991414865854\n",
      "{'mean_all': 7133.8198304607195, 'mean_3': 6767.9713317025025, 'median_all': 7479.078151152311, 'median_3': 6906.141367323291, 'rf': 7191.605074503928, 'rf_mean': 6623.286221745721}\n",
      "8 Best model is mean_all\n",
      "loss: 3893.869872130742\n",
      "val score: 4398.604684744836 + 1906.4946766746714\n",
      "{'mean_all': 3893.869872130742, 'mean_3': 3909.0519090519088, 'median_all': 4367.029367029367, 'median_3': 4897.624897624898, 'rf': 4259.183994730981, 'rf_mean': 4455.358627222214}\n",
      "9 Best model is median_all\n",
      "loss: 3696.9398819324524\n",
      "val score: 4320.641928876794 + 1810.9360805731185\n",
      "{'mean_all': 3722.9203006223483, 'mean_3': 3901.0213779901746, 'median_all': 3696.9398819324524, 'median_3': 3767.11778643428, 'rf': 3998.1344930914115, 'rf_mean': 4635.608824898802}\n",
      "10 Best model is median_3\n",
      "loss: 5672.786216442648\n",
      "val score: 4455.856357633379 + 1765.2441799436995\n",
      "{'mean_all': 6176.0435621987945, 'mean_3': 5801.406039412815, 'median_all': 6403.944924052016, 'median_3': 5672.786216442648, 'rf': 6605.635931251373, 'rf_mean': 8145.161780371574}\n",
      "11 Best model is median_3\n",
      "loss: 3527.2366522366524\n",
      "val score: 4371.436384415495 + 1704.134295146604\n",
      "{'mean_all': 4064.085635077483, 'mean_3': 3968.374218374219, 'median_all': 4142.992424242424, 'median_3': 3527.2366522366524, 'rf': 3859.8358161379315, 'rf_mean': 4679.63273658343}\n",
      "14 Best model is rf_mean\n",
      "loss: 3601.637200306545\n",
      "val score: 4307.286452406416 + 1645.3980995840213\n",
      "{'mean_all': 3681.795001612658, 'mean_3': 3879.347304214933, 'median_all': 3653.1556204734948, 'median_3': 3634.187409977076, 'rf': 3626.677071304641, 'rf_mean': 3601.637200306545}\n",
      "17 Best model is rf\n",
      "loss: 6274.375296513445\n",
      "val score: 4458.600978876188 + 1665.4823343177109\n",
      "{'mean_all': 8964.337376234025, 'mean_3': 8596.594013865328, 'median_all': 9730.794999358526, 'median_3': 9812.784802307404, 'rf': 6274.375296513445, 'rf_mean': 9542.245835008305}\n",
      "18 Best model is median_all\n",
      "loss: 10000.0\n",
      "val score: 4854.415194670745 + 2147.6489950039045\n",
      "{'mean_all': 12668.37956985968, 'mean_3': 10796.383758085887, 'median_all': 10000.0, 'median_3': 10000.0, 'rf': 11656.713224036996, 'rf_mean': 10000.690916304884}\n",
      "19 Best model is rf\n",
      "loss: 1756.6480573721446\n",
      "val score: 4647.897385517505 + 2214.0455285117528\n",
      "{'mean_all': 1785.146428170912, 'mean_3': 1782.3232296483043, 'median_all': 1826.3695056909362, 'median_3': 1990.1328856145628, 'rf': 1756.6480573721446, 'rf_mean': 2473.2613877353997}\n",
      "21 Best model is mean_all\n",
      "loss: 4931.242454669836\n",
      "val score: 4665.606452339526 + 2144.837270725522\n",
      "{'mean_all': 4931.242454669836, 'mean_3': 5306.9183417028435, 'median_all': 5676.8756549312075, 'median_3': 5267.279977136947, 'rf': 5294.157461590999, 'rf_mean': 9461.685113165298}\n",
      "22 Best model is rf\n",
      "loss: 2512.364278776415\n",
      "val score: 4538.945148012284 + 2141.590256878502\n",
      "{'mean_all': 2687.701728538741, 'mean_3': 2762.7443307641693, 'median_all': 2720.903443090736, 'median_3': 2755.615742104546, 'rf': 2512.364278776415, 'rf_mean': 2544.340634015554}\n",
      "23 Best model is rf_mean\n",
      "loss: 7194.683336962373\n",
      "val score: 4686.486158509511 + 2168.333526957484\n",
      "{'mean_all': 7597.814710592643, 'mean_3': 7763.530166447213, 'median_all': 7324.014990377798, 'median_3': 7431.3785070394, 'rf': 7822.004866953235, 'rf_mean': 7194.683336962373}\n",
      "24 Best model is mean_3\n",
      "loss: 3371.5760893755287\n",
      "val score: 4617.280365397196 + 2130.8270473667712\n",
      "{'mean_all': 3573.4172196516247, 'mean_3': 3371.5760893755287, 'median_all': 3699.826412071038, 'median_3': 3482.9750300440646, 'rf': 3419.799942380494, 'rf_mean': 3544.8435326416784}\n",
      "25 Best model is median_all\n",
      "loss: 2562.290980572362\n",
      "val score: 4514.530896155955 + 2124.6162772741063\n",
      "{'mean_all': 2634.566980378489, 'mean_3': 2884.157497468365, 'median_all': 2562.290980572362, 'median_3': 2605.498966071839, 'rf': 3178.918074209853, 'rf_mean': 2787.2298373560543}\n",
      "26 Best model is mean_3\n",
      "loss: 2663.3568272825332\n",
      "val score: 4426.379750019125 + 2110.5579134951954\n",
      "{'mean_all': 2779.414856437644, 'mean_3': 2663.3568272825332, 'median_all': 2775.333569750432, 'median_3': 2697.3713373819764, 'rf': 2741.5630303359117, 'rf_mean': 2723.8553014192967}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "train = pd.read_csv('train.csv.zip')\n",
    "train['DATE'] = pd.to_datetime(train['DATE'])\n",
    "train = train[train.DATE>=pd.datetime(2016, 8, 1)].reset_index(drop=True)\n",
    "\n",
    "def get_min_month(train):\n",
    "    \n",
    "    train = train[(train.DATE>=pd.datetime(2017, 1, 1 ))& (train.DATE<pd.datetime(2017, 8, 1 ))].reset_index(drop=True)\n",
    "    train['m'] = train.DATE.dt.month\n",
    "    return train[train.CLIENT_OUT == 0].groupby('m').CLIENT_OUT.count().argmin()\n",
    "\n",
    "min_mobth = get_min_month(train)\n",
    "# Unique ATMs\n",
    "ATM_IDs = train.ATM_ID.unique()\n",
    "def replace_week_ago(trainnine):\n",
    "    fist_q =  trainnine.CLIENT_OUT.quantile(0.9)\n",
    "    last_q = trainnine.CLIENT_OUT.quantile(0.1)\n",
    "    mask = (trainnine.CLIENT_OUT > fist_q) | (trainnine.CLIENT_OUT < last_q) \n",
    "    trainnine['mask'] = mask\n",
    "    trainnine['shifted'] = trainnine.CLIENT_OUT.shift(7)\n",
    "    trainnine.loc[mask,['CLIENT_OUT','shifted', ]] = trainnine.loc[mask,['shifted', 'CLIENT_OUT']].values\n",
    "    trainnine = trainnine.iloc[7:]\n",
    "    return trainnine[['DATE', \"ATM_ID\", 'CLIENT_OUT']]\n",
    "\n",
    "\n",
    "def apply_replace(train):\n",
    "    train2 = train.iloc[:0].copy()\n",
    "    ATM_IDs = train.ATM_ID.unique()\n",
    "    for ATM in ATM_IDs:\n",
    "        trainnine = train[(train.ATM_ID==ATM)].copy()\n",
    "        trainnine = replace_week_ago(trainnine)\n",
    "    #     print(trainnine.head())\n",
    "        train2 = pd.concat([train2, trainnine])\n",
    "    train = train2.copy()\n",
    "    del train2\n",
    "    return train\n",
    "train = apply_replace(train)\n",
    "train = apply_replace(train)\n",
    "train.columns = ['Timestamp','ForecastId','Value']\n",
    "\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "def feature_preprocessing(train, test, cat_cols, cat_type='ohe'):\n",
    "    \n",
    "    X_train, X_test = train[cat_cols].copy(), test[cat_cols].copy()\n",
    "\n",
    "    if (cat_type=='mean_enc'):\n",
    "        for j in ['mean', 'max', 'min', 'median']:\n",
    "            for i in cat_cols:\n",
    "                X_train[i], X_test[i] = MeanEncodingTransforming(X_train[i], train[['Value']], X_test[i], j)\n",
    "                X_train[i].columns = [i+'_'+j]\n",
    "                X_test[i].columns = [i+'_'+j]\n",
    "                \n",
    "    if (cat_type=='ohe'):\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        X_train = ohe.fit_transform(train[cat_cols])\n",
    "        X_test = ohe.transform(test[cat_cols])\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    return X_train, X_test \n",
    "\n",
    "def train_groupby(train, test, window, how):\n",
    "    mean_values = train[['Value', 'week_day']][-window:].groupby(['week_day']).agg(how).reset_index()\n",
    "    mean_values.columns = ['week_day', 'pred']\n",
    "    test = pd.merge(test, mean_values, how='left', on = ['week_day'])  \n",
    "    \n",
    "    return test['pred'].fillna(np.mean(train['Value']))\n",
    "\n",
    "def train_mean(train, window):\n",
    "    mean_value = np.mean(train['Value'].reset_index(drop=True)[-window:])\n",
    "    return mean_value\n",
    "\n",
    "def calc_score(pred, fact, index_mult):\n",
    "    return np.sum(abs(pred-fact)) / np.sum(fact) * 10000\n",
    "\n",
    "def train_rf(X_train, y_train, X_valid):\n",
    "\n",
    "    rf = RandomForestRegressor(max_features='sqrt', n_estimators=142, n_jobs=-1, random_state=4224)\n",
    "    rf.fit(X_train, y_train, sample_weight=np.linspace(0.5, 1, X_train.shape[0]) )\n",
    "    y_hat = rf.predict(X_valid)\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train_lgb(x_train, y_train, x_test):\n",
    "    verbose = 1\n",
    "    plot = 1\n",
    "    params = {\n",
    "    #     \"max_bin\": 1024,\n",
    "    #     \"learning_rate\": 0.01,\n",
    "        \"boosting_type\": \"goss\",\n",
    "        \"objective\": \"regression\",\n",
    "    #     'num_iterations':10000,\n",
    "#         \"metric\": \"auc\",\n",
    "    #     \"num_leaves\": 1000,\n",
    "#         \"verbose\": 1,\n",
    "    #     \"min_data\": 100,\n",
    "    #     \"boost_from_average\": True\n",
    "#         'early_stopping_round': 10,\n",
    "    }\n",
    "\n",
    "    d_train = lgb.Dataset(x_train, y_train)#, weight=np.linspace(0.5, 1, x_train.shape[0]))\n",
    "#     d_valid = lgb.Dataset(x_test, label=y_test)\n",
    "    model = lgb.train(params, d_train)\n",
    "\n",
    "    return model.predict(x_test)\n",
    "\n",
    "\n",
    "# train.columns = ['Timestamp','ForecastId','Value']\n",
    "def time_preprocess(X):\n",
    "    X['Timestamp'] = pd.to_datetime(X['Timestamp'])\n",
    "    X['year'] = X['Timestamp'].dt.year\n",
    "    X['month'] = X['Timestamp'].dt.month \n",
    "    X['day'] = X['Timestamp'].dt.day\n",
    "    X['week_day'] = X['Timestamp'].dt.weekday\n",
    "    X['hour'] = X['Timestamp'].dt.hour\n",
    "    X['minute'] = X['Timestamp'].dt.minute\n",
    "    X['minute'] = X['minute'] // 15 * 15\n",
    "    \n",
    "    return X\n",
    "\n",
    "# val = train[['ForecastId','Value']].groupby('ForecastId').diff()\n",
    "# val['Value'] = val['Value'].fillna(0)\n",
    "# val['Value'] = (val['Value']==0) * 1\n",
    "# val.columns = ['diff']\n",
    "# train = pd.concat([train, val], axis=1)\n",
    "\n",
    "# a = train[['ForecastId','diff']].groupby('ForecastId').apply(pd.rolling_sum, 7, min_periods=1)\n",
    "# a.columns = ['ForecastId', 'to_drop']\n",
    "# a = a['to_drop']\n",
    "# train = pd.concat([train, a], axis=1)\n",
    "\n",
    "# print(train.shape)\n",
    "# train = train[train['to_drop']<=3].reset_index(drop=True)\n",
    "# train = train[['Timestamp','ForecastId','Value']]\n",
    "train = time_preprocess(train)\n",
    "print(train.shape)\n",
    "\n",
    "train = train[train.year>=2016].reset_index(drop=True)\n",
    "train['year_month_weekday'] = train['year'].astype('str') + '_' + train['month'].astype('str') + '_' + train['week_day'].astype('str') \n",
    "features = train[['ForecastId', 'year_month_weekday', 'Value']].groupby(['ForecastId', 'year_month_weekday']).agg('mean').reset_index()\n",
    "features = features.pivot(index='ForecastId', columns='year_month_weekday', values='Value')\n",
    "features = features.fillna(0)\n",
    "\n",
    "\n",
    "losses = []\n",
    "iids = []\n",
    "\n",
    "sub = pd.DataFrame({})\n",
    "cat_cols = ['week_day','hour','minute','month']\n",
    "\n",
    "pred_dates  = ['2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19',\n",
    "               '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23',\n",
    "               '2017-08-24', '2017-08-25', '2017-08-26', '2017-08-27',\n",
    "               '2017-08-28', '2017-08-29', '2017-08-30', '2017-08-31',\n",
    "               '2017-09-01', '2017-09-02', '2017-09-03', '2017-09-04',\n",
    "               '2017-09-05', '2017-09-06', '2017-09-07', '2017-09-08',\n",
    "               '2017-09-09', '2017-09-10', '2017-09-11', '2017-09-12',\n",
    "               '2017-09-13', '2017-09-14', '2017-09-15', '2017-09-16',\n",
    "               '2017-09-17']\n",
    "\n",
    "index_mult = 0\n",
    "\n",
    "for i in ATM_IDs:\n",
    "    \n",
    "    # prepare train and test Dfs \n",
    "    days_in_train = 200\n",
    "    mask = ((train.Timestamp > pd.datetime(2017, 8, 10)) | (train.Timestamp < pd.datetime(2017, min_mobth, 1))) & (train['ForecastId']==i)\n",
    "    X_train = train[mask].reset_index(drop=True)[-days_in_train:].reset_index(drop=True)\n",
    "    X_train['Value'] = abs(X_train['Value'])\n",
    "       \n",
    "    X_train['Value_log'] = np.log1p(X_train['Value'])\n",
    "     \n",
    "    # prepare 'train_v' and 'valid_v' data frames - for validation \n",
    "    X_test = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    X_test = time_preprocess(X_test)\n",
    "    \n",
    "    obs_in_test = 35\n",
    "    X_train_v = X_train[:-obs_in_test].reset_index(drop=True)\n",
    "    X_valid_v = X_train[-obs_in_test:].reset_index(drop=True)\n",
    "    \n",
    "    # prepare features \n",
    "        # for train\n",
    "    X_train_ohe, X_test_ohe = feature_preprocessing(X_train, X_test, cat_cols, cat_type='ohe')\n",
    "  \n",
    "        # for validation \n",
    "    X_train_v_ohe, X_valid_v_ohe = feature_preprocessing(X_train_v, X_valid_v, cat_cols, cat_type='ohe')\n",
    "    \n",
    "        # group by mean\n",
    "    y_hat_grby_mean = train_groupby(X_train_v, X_valid_v, window=999999, how='mean')\n",
    "    y_hat_grby3_mean = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='mean')\n",
    "    \n",
    "        # group by median\n",
    "    y_hat_grby_median = train_groupby(X_train_v, X_valid_v, window=999999, how='median')\n",
    "    y_hat_grby3_median = train_groupby(X_train_v, X_valid_v, window=obs_in_test*3, how='median')\n",
    "    \n",
    "        # RandomForest \n",
    "    y_rf = train_rf(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "    y_rf_log = np.exp(train_rf(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    \n",
    "#     y_lgb = train_lgb(X_train_v_ohe, X_train_v['Value'], X_valid_v_ohe)\n",
    "#     y_lgb_log = np.exp(train_lgb(X_train_v_ohe, X_train_v['Value_log'], X_valid_v_ohe)) - 1\n",
    "    \n",
    "    score_grby_mean = calc_score(y_hat_grby_mean, X_valid_v['Value'], 0)\n",
    "    score_grby3_mean = calc_score(y_hat_grby3_mean, X_valid_v['Value'], 0)\n",
    "    score_grby_median = calc_score(y_hat_grby_median, X_valid_v['Value'], 0)\n",
    "    score_grby3_median = calc_score(y_hat_grby3_median, X_valid_v['Value'], 0)\n",
    "    \n",
    "    score_rf = calc_score(y_rf, X_valid_v['Value'], index_mult)\n",
    "    score_rf_log = calc_score(y_rf_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "#     score_lgb = calc_score(y_lgb, X_valid_v['Value'], index_mult)\n",
    "#     score_lgb_log = calc_score(y_lgb_log, X_valid_v['Value'], index_mult)\n",
    "    \n",
    "    \n",
    "    y_hats     = [y_hat_grby_mean, y_hat_grby3_mean, \n",
    "                  y_hat_grby_median, y_hat_grby3_median, \n",
    "                  y_rf, y_rf_log,\n",
    "#                   y_lgb, y_lgb_log\n",
    "                 ]\n",
    "    \n",
    "    all_scores = [score_grby_mean, score_grby3_mean, \n",
    "                  score_grby_median, score_grby3_median, \n",
    "                  score_rf, score_rf_log, \n",
    "#                   score_lgb, score_lgb_log\n",
    "                 ]\n",
    "    \n",
    "    best_score = np.min(all_scores)\n",
    "    \n",
    "    models_names = ['mean_all', 'mean_3', \n",
    "                    'median_all', 'median_3', \n",
    "                    'rf', 'rf_mean', \n",
    "#                     'lgb', 'lgb_mean'\n",
    "                   ]\n",
    "    \n",
    "    losses.append(best_score)\n",
    "    \n",
    "    temp_df = pd.DataFrame({'Timestamp':pred_dates, 'ForecastId':i})\n",
    "    temp_df = time_preprocess(temp_df)\n",
    "    \n",
    "    if (score_grby_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='mean')\n",
    "    \n",
    "    if (score_grby3_mean==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='mean')\n",
    "        \n",
    "    if (score_grby_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=999999, how='median')\n",
    "    \n",
    "    if (score_grby3_median==best_score):\n",
    "        y_hat = train_groupby(X_train, X_test, window=obs_in_test*3, how='median')\n",
    "\n",
    "    if (score_rf==best_score):\n",
    "        y_hat = train_rf(X_train_ohe, X_train['Value'], X_test_ohe)\n",
    "        \n",
    "    if (score_rf_log==best_score):\n",
    "        y_hat = np.exp(train_rf(X_train_ohe, X_train['Value_log'], X_test_ohe)) -1    \n",
    "\n",
    "#     if (score_lgb==best_score):\n",
    "#         y_hat = train_lgb(X_train_ohe, X_train['Value'], X_test_ohe)\n",
    "        \n",
    "#     if (score_lgb_log==best_score):\n",
    "#         y_hat = np.exp(train_lgb(X_train_ohe, X_train['Value_log'], X_test_ohe)) -1    \n",
    "        \n",
    "    X_test['Value'] = y_hat\n",
    "    sub = pd.concat([sub,X_test],axis=0)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "    ax.plot(y_hats[np.argmin(all_scores)])\n",
    "    ax.plot(X_valid_v['Value'].reset_index(drop=True))\n",
    "    fig.savefig('pics/'+str(i)+'.jpg')   # save the figure to file\n",
    "    # plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(i, 'Best model is', models_names[np.argmin(all_scores)])\n",
    "    print('loss:', np.min(all_scores) )\n",
    "    print('val score:', np.mean(losses),'+',np.std(losses) )\n",
    "    print(dict(zip(models_names, all_scores)))\n",
    "    print('-------------------------------')\n",
    "\n",
    "sub = sub[['Timestamp', 'ForecastId', 'Value']]\n",
    "sub.columns = ['DATE', 'ATM_ID', 'CLIENT_OUT']\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T18:50:32.162662Z",
     "start_time": "2018-04-03T18:50:31.401386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m\n",
       "1    1663\n",
       "2     942\n",
       "3     807\n",
       "4     759\n",
       "5     944\n",
       "6     829\n",
       "7     895\n",
       "Name: CLIENT_OUT, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date, timedelta\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "train = pd.read_csv('train.csv.zip')\n",
    "train['DATE'] = pd.to_datetime(train['DATE'])\n",
    "def get_min_month(train):\n",
    "    \n",
    "    train = train[(train.DATE>=pd.datetime(2017, 1, 1 ))& (train.DATE<pd.datetime(2017, 8, 1 ))].reset_index(drop=True)\n",
    "    train['m'] = train.DATE.dt.month\n",
    "    return train[train.CLIENT_OUT == 0].groupby('m').CLIENT_OUT.count().argmin()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
